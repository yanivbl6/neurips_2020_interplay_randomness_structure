{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1597323763938,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "2MXJ0aADKNRN",
    "outputId": "b5f6505f-d540-4e9b-c505-a65370f69af4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model_LSTM import RNN\n",
    "\n",
    "# Data directory for saving\n",
    "from data_dir import data_dir as model_dir\n",
    "# Directory for datasets and pretrained word vectors\n",
    "datasets_dir = os.path.join(model_dir, 'sentiment_analysis/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1597323763938,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "2MXJ0aADKNRN",
    "outputId": "b5f6505f-d540-4e9b-c505-a65370f69af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save model to \n",
      "  '../data/sst_lstm_nlayers_2_nhid_256_emb_pretrained_fix_dim_100_dropout_05_weight_decay_0000_seed_1371.pt'\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# TSM: Tai, Socher, Manning, 2015 for SST-2\n",
    "# Choose dataset\n",
    "DATASET = ['SST', 'IMDB'][0]\n",
    "# Choose pretrained embedding? (GloVe)\n",
    "PRETRAINED_EMB = True       # TSM: T\n",
    "# Scale pretrained embedding by 1/sqrt(N)\n",
    "SCALE_EMB = False\n",
    "# Train the embedding?\n",
    "TRAIN_EMB = False           # TSM: T\n",
    "\n",
    "# Data set and training parameters\n",
    "if DATASET == 'SST':\n",
    "    MAX_VOCAB_SIZE = None   # max: 15431; TSM: None\n",
    "    N_LAYERS = 2            # TSM: 1, 2\n",
    "\n",
    "    RNN_TYPE = 'LSTM'       # TSM: LSTM\n",
    "    BATCH_SIZE = 64         # TSM: 25 (paper) or 5 (github repo)\n",
    "    EMB_DIM = [50, 100, 200, 300][1] # TSM: 300\n",
    "    HIDDEN_DIM = 256       # TSM: 150\n",
    "    G_REC = None\n",
    "    N_EPOCHS = 500\n",
    "    DROPOUT = 0.5\n",
    "\n",
    "    # RNN_TYPE = 'RNN'\n",
    "    # BATCH_SIZE = 64\n",
    "    # EMB_DIM = [50, 100, 200, 300][1] \n",
    "    # HIDDEN_DIM = 1024\n",
    "    # G_REC = None\n",
    "    # N_EPOCHS = 200\n",
    "    # DROPOUT = 0.0 \n",
    "\n",
    "elif DATASET == 'IMDB':\n",
    "    MAX_VOCAB_SIZE = 25_000 # for IMDB\n",
    "    BATCH_SIZE = 64\n",
    "    HIDDEN_DIM = 1024\n",
    "    N_LAYERS = 2\n",
    "    RNN_TYPE = 'LSTM'\n",
    "    N_EPOCHS = 500\n",
    "    EMB_DIM = [50, 100, 200, 300][3]\n",
    "    G_REC = None\n",
    "    DROPOUT = 0.2           # TSM: 0.5\n",
    "# Network parameters\n",
    "BIDIRECTIONAL = False       # TSM: F, T\n",
    "WEIGHT_DECAY = 0            # TSM: 0\n",
    "\n",
    "# Initial weight statistics from previous training?\n",
    "LOAD_WB_STAT = False\n",
    "\n",
    "# Use binary or multiple labels? \n",
    "# Use multiple for SST with neutral labels or SST-5: fine-grained sentiments).\n",
    "BINARY_LABELS = True\n",
    "\n",
    "# Random seed\n",
    "SEED = random.randint(0, 9999)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "###########################################################################\n",
    "# File name for saved model (model.pt)\n",
    "if G_REC is not None:\n",
    "    g_str = ''.join(('_g_%.1f'%G_REC).split('.'))\n",
    "# Embedding\n",
    "emb_str = '_emb'\n",
    "if PRETRAINED_EMB:\n",
    "    emb_str += \"_pretrained\"\n",
    "else:\n",
    "    emb_str += \"_random\"\n",
    "if SCALE_EMB:\n",
    "    emb_str += \"_scaled\"\n",
    "if TRAIN_EMB:\n",
    "    emb_str += \"_train\"\n",
    "else:\n",
    "    emb_str += \"_fix\"\n",
    "emb_str += '_dim_%d'%EMB_DIM\n",
    "# Regularization\n",
    "dropout_str = ''.join(('_dropout_%.1f'%DROPOUT).split('.'))\n",
    "weight_decay_str = ''.join(('_weight_decay_%.3f'%WEIGHT_DECAY).split('.'))\n",
    "# Join\n",
    "model_name = (DATASET.lower() \n",
    "              + ('_nvocab_' + str(MAX_VOCAB_SIZE) \n",
    "                 if MAX_VOCAB_SIZE is not None else '')\n",
    "              + '_' + RNN_TYPE.lower()\n",
    "              + '_nlayers_%d'%N_LAYERS \n",
    "              + '_nhid_%d'%HIDDEN_DIM \n",
    "              + (g_str if G_REC is not None else '')\n",
    "              + emb_str\n",
    "              + dropout_str\n",
    "              + weight_decay_str\n",
    "              + '_seed_%d'%SEED)\n",
    "model_name += '.pt'\n",
    "SAVE = os.path.join(model_dir, model_name)\n",
    "print(\"Will save model to \\n  '%s'\" % SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6022,
     "status": "ok",
     "timestamp": 1597323775100,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "lAb4oCOsRrxx",
    "outputId": "ca2322c8-33b1-4bf8-d426-55ac402cf4a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/example.py:94: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/example.py:94: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training examples: 6920\n",
      "Number of validation examples: 872\n",
      "Number of testing examples: 1821\n",
      "\n",
      "Total number of words in training dataset: 138,162\n",
      "Average sentence length: 20.0 words.\n",
      "\n",
      "Example sentence (len = 39):\n",
      "The Rock is destined to be the 21st Century 's new ` ` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean - Claud Van Damme or Steven Segal .\n"
     ]
    }
   ],
   "source": [
    "# Define how the dataset is split. \n",
    "# Tokenize = act of splitting the string into discrete 'tokens'.\n",
    "# Include length for packed padded sequences\n",
    "TEXT = torchtext.legacy.data.Field(tokenize ='spacy', include_lengths=True)\n",
    "if BINARY_LABELS:\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "else:\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "# Load dataset (downloads first if not provided).\n",
    "if DATASET == 'SST':\n",
    "    train_data, valid_data, test_data = datasets.SST.splits(TEXT, LABEL, \n",
    "        root=datasets_dir, fine_grained=False, filter_pred=lambda ex: ex.label != 'neutral')\n",
    "elif DATASET == 'IMDB':\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=datasets_dir)\n",
    "    print('   Loading dataset complete.')\n",
    "    # Split train into train and validation set\n",
    "    train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Understand the dataset\n",
    "print(f'\\nNumber of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "# Total number of words in the entire dataset (counting doubles)\n",
    "n_total_words = 0\n",
    "for ex in train_data.examples:\n",
    "    n_total_words += len(ex.text)\n",
    "print(f'\\nTotal number of words in training dataset: {n_total_words:,}')\n",
    "print(\"Average sentence length: %.1f words.\" % (n_total_words / len(train_data)))\n",
    "\n",
    "# One example sentence\n",
    "ex = train_data.examples[0].text\n",
    "print(\"\\nExample sentence (len = %d):\" % len(ex))\n",
    "print(' '.join(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6082,
     "status": "ok",
     "timestamp": 1597323776642,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "KQDIWM50-uOe",
    "outputId": "fe8b6ff5-e1d8-4955-d65f-f292f4c866c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full length of vocab without capping at MAX_VOCAB_SIZE: 15,431\n",
      "Unique tokens in TEXT vocabulary: 15,433\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "LABELS: defaultdict(None, {'positive': 0, 'negative': 1})\n",
      "\n",
      "First 10 words of entire vocab:\n",
      "[['.' '6552']\n",
      " [',' '5883']\n",
      " ['the' '4974']\n",
      " ['and' '3827']\n",
      " ['of' '3672']\n",
      " ['a' '3623']\n",
      " ['to' '2441']\n",
      " ['-' '2202']\n",
      " ['is' '2080']\n",
      " [\"'s\" '2025']]\n",
      "\n",
      "Last 5 words included in the vocab:\n",
      "[['Staggeringly' '1']\n",
      " ['hippie' '1']\n",
      " ['yuppie' '1']\n",
      " ['muttering' '1']\n",
      " ['dissing' '1']]\n",
      "\n",
      "Last 5 words of entire vocab:\n",
      "[['Staggeringly' '1']\n",
      " ['hippie' '1']\n",
      " ['yuppie' '1']\n",
      " ['muttering' '1']\n",
      " ['dissing' '1']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary from training data. \n",
    "# Limit the max size of vocabulary (otherwise > 100,000).\n",
    "\n",
    "# # Randomly initialized:\n",
    "if PRETRAINED_EMB:\n",
    "    # Pretrained word embeddings (~1 GB to download!)\n",
    "    TEXT.build_vocab(train_data, \n",
    "                     max_size=MAX_VOCAB_SIZE, \n",
    "                     vectors=\"glove.6B.%dd\"%EMB_DIM, \n",
    "                     vectors_cache=datasets_dir,\n",
    "                     unk_init=torch.Tensor.normal_)\n",
    "else:\n",
    "    TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Create iterators over datasets\n",
    "# The BucketIterator will return batches with examples of similar lengths to minimize padding. \n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    device=device)\n",
    "\n",
    "# For the vocabulary, note that there are 2 extra tokens, for <unk> and <pad>.\n",
    "print(f\"Full length of vocab without capping at \"\n",
    "      + f\"MAX_VOCAB_SIZE: {len(TEXT.vocab.freqs.most_common()):,}\")\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab):,}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab):,}\")\n",
    "print(\"LABELS:\", LABEL.vocab.stoi)\n",
    "\n",
    "assert BINARY_LABELS == (len(LABEL.vocab.stoi) == 2), '\\n\\nLabels are not binary!\\n'\n",
    "\n",
    "# Print most and least common words\n",
    "n_show = 10\n",
    "print(\"\\nFirst %d words of entire vocab:\"%n_show)\n",
    "print(np.array(TEXT.vocab.freqs.most_common(n_show)))\n",
    "n_show = 5\n",
    "print(\"\\nLast %d words included in the vocab:\"%n_show)\n",
    "print(np.array(TEXT.vocab.freqs.most_common(MAX_VOCAB_SIZE)[-n_show:]))\n",
    "print(\"\\nLast %d words of entire vocab:\"%n_show)\n",
    "print(np.array(TEXT.vocab.freqs.most_common()[-n_show:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5837,
     "status": "ok",
     "timestamp": 1597323776643,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "Vye2ZWQWXbe5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5479,
     "status": "ok",
     "timestamp": 1597323776644,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "uT-S2nsb-uOq",
    "outputId": "a4cb1dc1-e9ba-4120-d734-25395aedecd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model has 893,185 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Create RNN instance\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "if BINARY_LABELS:\n",
    "    OUTPUT_DIM = 1\n",
    "else:\n",
    "    OUTPUT_DIM = len(LABEL.vocab)\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# Instantiate\n",
    "model = RNN(RNN_TYPE, INPUT_DIM, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, \n",
    "            BIDIRECTIONAL, DROPOUT, PAD_IDX, TRAIN_EMB)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'\\nThe model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# Embedding\n",
    "if PRETRAINED_EMB:\n",
    "    # Apply pretrained embeddings\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.encoder.weight.data.copy_(pretrained_embeddings)\n",
    "if SCALE_EMB:\n",
    "    model.encoder.weight.data *= 1 / np.sqrt(HIDDEN_DIM)\n",
    "# Initialize <unk> and <pad> to zero:\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.encoder.weight.data[UNK_IDX] = torch.zeros(EMB_DIM)\n",
    "model.encoder.weight.data[PAD_IDX] = torch.zeros(EMB_DIM)\n",
    "# print(model.embedding.weight.data)\n",
    "\n",
    "# RNN and decoder\n",
    "if LOAD_WB_STAT:\n",
    "    # Choose file\n",
    "    wb_stat_name = 'sst_emb_pretrained_scaled_fix_dim_300_lstm_nlayers_1_nhid_150_dropout_05_seed_1234'\n",
    "    # Load and set weights\n",
    "    wb_stat_file = os.path.join(model_dir, wb_stat_name)\n",
    "    with open(args.wb_stat_file, 'rb') as f:\n",
    "        wb_stat = pickle.load(f)  \n",
    "    for key, param in model.named_parameters():\n",
    "        if 'encoder' in key:\n",
    "            # Leave as is!\n",
    "            pass \n",
    "        elif 'rnn' in key:\n",
    "            # RNN weights and biases come in blocks of n_rnn * N x N. Separate these blocks.\n",
    "            for k, key_rnn in enumerate(model.keys_rnn):\n",
    "                wb_mean, wb_std = wb_stat[key][key_rnn]\n",
    "                mul = param.shape[0] // model.n_rnn\n",
    "                sub_w = param[k*mul : (k+1)*mul]\n",
    "                sub_w.data.normal_(wb_mean, wb_std)\n",
    "                print(key, key_rnn, ' ', wb_mean, wb_std)\n",
    "        elif 'decoder' in key:\n",
    "            wb_mean, wb_std = wb_stat[key]\n",
    "            param.data.normal_(wb_mean, wb_std)\n",
    "            print(key, ' ', wb_mean, wb_std)\n",
    "elif G_REC is not None:\n",
    "    # Recurrent weights\n",
    "    for key, param in model.rnn.named_parameters():\n",
    "        if 'weight' in key:\n",
    "            # Input vs. recurrent\n",
    "            hh_or_ih, layer_str = key.split('_')[1:]\n",
    "            layer = int(layer_str[1:])\n",
    "            # Input layer: scale by 1/sqrt(embedding_dim)\n",
    "            print(hh_or_ih, layer)\n",
    "            if hh_or_ih == 'ih' and layer == 0:\n",
    "                param.data.normal_(0, 1. / np.sqrt(model.embedding_dim))\n",
    "            else:\n",
    "                # LSTM and GRU cells have combined weights for the effect of the\n",
    "                # last hidden state on the new state and the gates. \n",
    "                # We scale only the hidden-to-hidden weights ('hh') by\n",
    "                # a factor g controlling the radius of the spectrum.\n",
    "                for k, key_rnn in enumerate(model.keys_rnn):\n",
    "                    mul = param.shape[0] // model.n_states\n",
    "                    sub_w = param[k*mul : (k+1)*mul]\n",
    "                    if hh_or_ih == 'hh' and key_rnn == 'c':\n",
    "                        # Scale only the recurrent state weights by g\n",
    "                        sub_w.data.normal_(0, G_REC / np.sqrt(model.hidden_dim))\n",
    "                    else:\n",
    "                        sub_w.data.normal_(0, 1. / np.sqrt(model.hidden_dim))\n",
    "        else:\n",
    "            # Biases are set to zero by default. \n",
    "            pass\n",
    "            \n",
    "        # # Decoder: leave as is...\n",
    "        # decoder_max = math.sqrt(3 / model.hidden_dim)\n",
    "        # model.decoder.weight.data.uniform_(-decoder_max, decoder_max)\n",
    "        # model.decoder.bias.data.zero_()\n",
    "else:\n",
    "    # Use standard initialization, but rescale the input weights of layer 0. \n",
    "    # model.rnn.weight_ih_l0.data.uniform_(0, 1. / np.sqrt(model.embedding_dim))\n",
    "    a = 1. / np.sqrt(model.embedding_dim)\n",
    "    model.rnn.weight_ih_l0.data.uniform_(-a, a)\n",
    "\n",
    "# Save the initial model connectivity\n",
    "state_dict_init = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Move model to GPU before choosing the optimizer\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5245,
     "status": "ok",
     "timestamp": 1597323776644,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "j4PjVbDGCXlv"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Choose optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=0.05)\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Learning rate for adam should be scaled by network size!\n",
    "lr0 = 0.01\n",
    "lr = lr0 / HIDDEN_DIM\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "if BINARY_LABELS:\n",
    "    # Binary cross-entropy loss with logits\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    # Binary accuracy\n",
    "    def accuracy(preds, y):\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "else:\n",
    "    # Cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # Categorical accuracy\n",
    "    def accuracy(preds, y):\n",
    "        \"\"\" Categorical accuracy for multiple classes.\"\"\"\n",
    "        max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def train_fwd(model, iterator, optimizer, criterion, n_directions = 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for _ in range(n_directions):\n",
    "            predictions, loss = model.fwd_mode(x = batch.text,y= batch.label, loss= lambda x,y: criterion(x, y), mage = False)\n",
    "        \n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "RB39dWFDT4L_",
    "outputId": "e1316be3-8a53-45dc-e51b-2a82e50d415d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'RNN' object has no attribute 'fwd_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31f51e7abab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bc6ab5686e4b>\u001b[0m in \u001b[0;36mtrain_fwd\u001b[0;34m(model, iterator, optimizer, criterion, n_directions)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_directions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfwd_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'RNN' object has no attribute 'fwd_mode'"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "best_valid_loss = float('inf')\n",
    "train_losses, valid_losses = np.zeros((2, N_EPOCHS))\n",
    "loss_acc = np.zeros((N_EPOCHS, 4))\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_fwd(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Save losses and accuracy\n",
    "    loss_acc[epoch] = train_loss, valid_loss, train_acc, valid_acc\n",
    "\n",
    "    # # Save the best model so far\n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     state_dict_best = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    show_step = N_EPOCHS // 20\n",
    "    if (epoch + 1) % show_step == 0 or epoch == 0:\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        # Save\n",
    "        state_dict_final = model.state_dict()\n",
    "        with open(SAVE, 'wb') as f:\n",
    "            torch.save({'state_dict_init': state_dict_init,\n",
    "                        'state_dict_final': state_dict_final,\n",
    "                        'loss_acc': loss_acc,\n",
    "                        }, f)\n",
    "\n",
    "\n",
    "#######################################################################        \n",
    "# Save the initial and last model\n",
    "state_dict_final = model.state_dict()\n",
    "with open(SAVE, 'wb') as f:\n",
    "    torch.save({'state_dict_init': state_dict_init,\n",
    "                'state_dict_final': state_dict_final,\n",
    "                'loss_acc': loss_acc,\n",
    "                }, f)\n",
    "# print(\"Saved last model to '%s'\" % SAVE)\n",
    "print(\"Saved initial and final model to '%s'\" % SAVE)\n",
    "# print(\"Saved best model to '%s'\" % SAVE)\n",
    "\n",
    "# Save the initial and last model\n",
    "state_dict_final = model.state_dict()\n",
    "with open(SAVE, 'wb') as f:\n",
    "    torch.save({'state_dict_init': state_dict_init,\n",
    "                'state_dict_final': state_dict_final,\n",
    "                'loss_acc': loss_acc,\n",
    "                }, f)\n",
    "# print(\"Saved last model to '%s'\" % SAVE)\n",
    "print(\"Saved initial and final model to '%s'\" % SAVE)\n",
    "\n",
    "# Plot loss and accuracy over training epochs\n",
    "fig = plt.figure(figsize=(6, 6), facecolor='w')\n",
    "axes = fig.subplots(2, 1)\n",
    "axes = axes[:, None]\n",
    "epochs = np.arange(N_EPOCHS)\n",
    "train_loss, valid_loss, train_acc, valid_acc = loss_acc.T\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs, train_loss, label='train')\n",
    "ax.plot(epochs, valid_loss, label='valid')\n",
    "ax.axhline(0, c='0.5', zorder=-1)\n",
    "ax.legend()\n",
    "ax.set_ylabel('Loss')\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs, train_acc, label='train')\n",
    "ax.plot(epochs, valid_acc, label='valid')\n",
    "ax.axhline(0, c='0.5', zorder=-1)\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nN4y_EfmUSqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FSmXtxwgGnO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKX2UIA8Ll0B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "0 2 2\n",
      "0 3 3\n",
      "0 4 4\n",
      "0 5 5\n",
      "0 6 6\n",
      "0 7 7\n",
      "0 8 8\n",
      "0 9 9\n",
      "0 10 10\n",
      "0 11 11\n",
      "0 12 12\n",
      "0 13 13\n",
      "0 14 14\n",
      "0 15 15\n",
      "0 16 16\n",
      "0 17 17\n",
      "0 18 18\n",
      "0 19 19\n",
      "0 20 20\n",
      "0 21 21\n",
      "0 22 22\n",
      "0 23 23\n",
      "0 24 24\n",
      "0 25 25\n",
      "0 26 26\n",
      "0 27 27\n",
      "0 28 28\n",
      "0 29 29\n",
      "0 30 30\n",
      "0 31 31\n",
      "0 32 32\n",
      "0 33 33\n",
      "0 34 34\n",
      "0 35 35\n",
      "0 36 36\n",
      "0 37 37\n",
      "0 38 38\n",
      "0 39 39\n",
      "0 40 40\n",
      "0 41 41\n",
      "0 42 42\n",
      "0 43 43\n",
      "0 44 44\n",
      "0 45 45\n",
      "0 46 46\n",
      "0 47 47\n",
      "0 48 48\n",
      "0 49 49\n",
      "0 50 50\n",
      "0 51 51\n",
      "0 52 52\n",
      "0 53 53\n",
      "0 54 54\n",
      "0 55 55\n",
      "0 56 56\n",
      "0 57 57\n",
      "0 58 58\n",
      "0 59 59\n",
      "0 60 60\n",
      "0 61 61\n",
      "0 62 62\n",
      "0 63 63\n",
      "0 64 64\n",
      "0 65 65\n",
      "0 66 66\n",
      "0 67 67\n",
      "0 68 68\n",
      "0 69 69\n",
      "0 70 70\n",
      "0 71 71\n",
      "0 72 72\n",
      "0 73 73\n",
      "0 74 74\n",
      "0 75 75\n",
      "0 76 76\n",
      "0 77 77\n",
      "0 78 78\n",
      "0 79 79\n",
      "0 80 80\n",
      "0 81 81\n",
      "0 82 82\n",
      "0 83 83\n",
      "0 84 84\n",
      "0 85 85\n",
      "0 86 86\n",
      "0 87 87\n",
      "0 88 88\n",
      "0 89 89\n",
      "0 90 90\n",
      "0 91 91\n",
      "0 92 92\n",
      "0 93 93\n",
      "0 94 94\n",
      "0 95 95\n",
      "0 96 96\n",
      "0 97 97\n",
      "0 98 98\n",
      "0 99 99\n",
      "0 100 100\n",
      "0 101 101\n",
      "0 102 102\n",
      "0 103 103\n",
      "0 104 104\n",
      "0 105 105\n",
      "0 106 106\n",
      "0 107 107\n",
      "0 108 108\n",
      "0 109 109\n",
      "0 110 110\n",
      "0 111 111\n",
      "0 112 112\n",
      "0 113 113\n",
      "0 114 114\n",
      "0 115 115\n",
      "0 116 116\n",
      "0 117 117\n",
      "0 118 118\n",
      "0 119 119\n",
      "0 120 120\n",
      "0 121 121\n",
      "0 122 122\n",
      "0 123 123\n",
      "0 124 124\n",
      "0 125 125\n",
      "0 126 126\n",
      "0 127 127\n",
      "0 128 128\n",
      "0 129 129\n",
      "0 130 130\n",
      "0 131 131\n",
      "0 132 132\n",
      "0 133 133\n",
      "0 134 134\n",
      "0 135 135\n",
      "0 136 136\n",
      "0 137 137\n",
      "0 138 138\n",
      "0 139 139\n",
      "0 140 140\n",
      "0 141 141\n",
      "0 142 142\n",
      "0 143 143\n",
      "0 144 144\n",
      "0 145 145\n",
      "0 146 146\n",
      "0 147 147\n",
      "0 148 148\n",
      "0 149 149\n",
      "0 150 150\n",
      "0 151 151\n",
      "0 152 152\n",
      "0 153 153\n",
      "0 154 154\n",
      "0 155 155\n",
      "0 156 156\n",
      "0 157 157\n",
      "0 158 158\n",
      "0 159 159\n",
      "0 160 160\n",
      "0 161 161\n",
      "0 162 162\n",
      "0 163 163\n",
      "0 164 164\n",
      "0 165 165\n",
      "0 166 166\n",
      "0 167 167\n",
      "0 168 168\n",
      "0 169 169\n",
      "0 170 170\n",
      "0 171 171\n",
      "0 172 172\n",
      "0 173 173\n",
      "0 174 174\n",
      "0 175 175\n",
      "0 176 176\n",
      "0 177 177\n",
      "0 178 178\n",
      "0 179 179\n",
      "0 180 180\n",
      "0 181 181\n",
      "0 182 182\n",
      "0 183 183\n",
      "0 184 184\n",
      "0 185 185\n",
      "0 186 186\n",
      "0 187 187\n",
      "0 188 188\n",
      "0 189 189\n",
      "0 190 190\n",
      "0 191 191\n",
      "0 192 192\n",
      "0 193 193\n",
      "0 194 194\n",
      "0 195 195\n",
      "0 196 196\n",
      "0 197 197\n",
      "0 198 198\n",
      "0 199 199\n",
      "0 200 200\n",
      "0 201 201\n",
      "0 202 202\n",
      "0 203 203\n",
      "0 204 204\n",
      "0 205 205\n",
      "0 206 206\n",
      "0 207 207\n",
      "0 208 208\n",
      "0 209 209\n",
      "0 210 210\n",
      "0 211 211\n",
      "0 212 212\n",
      "0 213 213\n",
      "0 214 214\n",
      "0 215 215\n",
      "0 216 216\n",
      "0 217 217\n",
      "0 218 218\n",
      "0 219 219\n",
      "0 220 220\n",
      "0 221 221\n",
      "0 222 222\n",
      "0 223 223\n",
      "0 224 224\n",
      "0 225 225\n",
      "0 226 226\n",
      "0 227 227\n",
      "0 228 228\n",
      "0 229 229\n",
      "0 230 230\n",
      "0 231 231\n",
      "0 232 232\n",
      "0 233 233\n",
      "0 234 234\n",
      "0 235 235\n",
      "0 236 236\n",
      "0 237 237\n",
      "0 238 238\n",
      "0 239 239\n",
      "0 240 240\n",
      "0 241 241\n",
      "0 242 242\n",
      "0 243 243\n",
      "0 244 244\n",
      "0 245 245\n",
      "0 246 246\n",
      "0 247 247\n",
      "0 248 248\n",
      "0 249 249\n",
      "0 250 250\n",
      "0 251 251\n",
      "0 252 252\n",
      "0 253 253\n",
      "0 254 254\n",
      "0 255 255\n",
      "1 0 0\n",
      "1 1 1\n",
      "1 2 2\n",
      "1 3 3\n",
      "1 4 4\n",
      "1 5 5\n",
      "1 6 6\n",
      "1 7 7\n",
      "1 8 8\n",
      "1 9 9\n",
      "1 10 10\n",
      "1 11 11\n",
      "1 12 12\n",
      "1 13 13\n",
      "1 14 14\n",
      "1 15 15\n",
      "1 16 16\n",
      "1 17 17\n",
      "1 18 18\n",
      "1 19 19\n",
      "1 20 20\n",
      "1 21 21\n",
      "1 22 22\n",
      "1 23 23\n",
      "1 24 24\n",
      "1 25 25\n",
      "1 26 26\n",
      "1 27 27\n",
      "1 28 28\n",
      "1 29 29\n",
      "1 30 30\n",
      "1 31 31\n",
      "1 32 32\n",
      "1 33 33\n",
      "1 34 34\n",
      "1 35 35\n",
      "1 36 36\n",
      "1 37 37\n",
      "1 38 38\n",
      "1 39 39\n",
      "1 40 40\n",
      "1 41 41\n",
      "1 42 42\n",
      "1 43 43\n",
      "1 44 44\n",
      "1 45 45\n",
      "1 46 46\n",
      "1 47 47\n",
      "1 48 48\n",
      "1 49 49\n",
      "1 50 50\n",
      "1 51 51\n",
      "1 52 52\n",
      "1 53 53\n",
      "1 54 54\n",
      "1 55 55\n",
      "1 56 56\n",
      "1 57 57\n",
      "1 58 58\n",
      "1 59 59\n",
      "1 60 60\n",
      "1 61 61\n",
      "1 62 62\n",
      "1 63 63\n",
      "1 64 64\n",
      "1 65 65\n",
      "1 66 66\n",
      "1 67 67\n",
      "1 68 68\n",
      "1 69 69\n",
      "1 70 70\n",
      "1 71 71\n",
      "1 72 72\n",
      "1 73 73\n",
      "1 74 74\n",
      "1 75 75\n",
      "1 76 76\n",
      "1 77 77\n",
      "1 78 78\n",
      "1 79 79\n",
      "1 80 80\n",
      "1 81 81\n",
      "1 82 82\n",
      "1 83 83\n",
      "1 84 84\n",
      "1 85 85\n",
      "1 86 86\n",
      "1 87 87\n",
      "1 88 88\n",
      "1 89 89\n",
      "1 90 90\n",
      "1 91 91\n",
      "1 92 92\n",
      "1 93 93\n",
      "1 94 94\n",
      "1 95 95\n",
      "1 96 96\n",
      "1 97 97\n",
      "1 98 98\n",
      "1 99 99\n",
      "1 100 100\n",
      "1 101 101\n",
      "1 102 102\n",
      "1 103 103\n",
      "1 104 104\n",
      "1 105 105\n",
      "1 106 106\n",
      "1 107 107\n",
      "1 108 108\n",
      "1 109 109\n",
      "1 110 110\n",
      "1 111 111\n",
      "1 112 112\n",
      "1 113 113\n",
      "1 114 114\n",
      "1 115 115\n",
      "1 116 116\n",
      "1 117 117\n",
      "1 118 118\n",
      "1 119 119\n",
      "1 120 120\n",
      "1 121 121\n",
      "1 122 122\n",
      "1 123 123\n",
      "1 124 124\n",
      "1 125 125\n",
      "1 126 126\n",
      "1 127 127\n",
      "1 128 128\n",
      "1 129 129\n",
      "1 130 130\n",
      "1 131 131\n",
      "1 132 132\n",
      "1 133 133\n",
      "1 134 134\n",
      "1 135 135\n",
      "1 136 136\n",
      "1 137 137\n",
      "1 138 138\n",
      "1 139 139\n",
      "1 140 140\n",
      "1 141 141\n",
      "1 142 142\n",
      "1 143 143\n",
      "1 144 144\n",
      "1 145 145\n",
      "1 146 146\n",
      "1 147 147\n",
      "1 148 148\n",
      "1 149 149\n",
      "1 150 150\n",
      "1 151 151\n",
      "1 152 152\n",
      "1 153 153\n",
      "1 154 154\n",
      "1 155 155\n",
      "1 156 156\n",
      "1 157 157\n",
      "1 158 158\n",
      "1 159 159\n",
      "1 160 160\n",
      "1 161 161\n",
      "1 162 162\n",
      "1 163 163\n",
      "1 164 164\n",
      "1 165 165\n",
      "1 166 166\n",
      "1 167 167\n",
      "1 168 168\n",
      "1 169 169\n",
      "1 170 170\n",
      "1 171 171\n",
      "1 172 172\n",
      "1 173 173\n",
      "1 174 174\n",
      "1 175 175\n",
      "1 176 176\n",
      "1 177 177\n",
      "1 178 178\n",
      "1 179 179\n",
      "1 180 180\n",
      "1 181 181\n",
      "1 182 182\n",
      "1 183 183\n",
      "1 184 184\n",
      "1 185 185\n",
      "1 186 186\n",
      "1 187 187\n",
      "1 188 188\n",
      "1 189 189\n",
      "1 190 190\n",
      "1 191 191\n",
      "1 192 192\n",
      "1 193 193\n",
      "1 194 194\n",
      "1 195 195\n",
      "1 196 196\n",
      "1 197 197\n",
      "1 198 198\n",
      "1 199 199\n",
      "1 200 200\n",
      "1 201 201\n",
      "1 202 202\n",
      "1 203 203\n",
      "1 204 204\n",
      "1 205 205\n",
      "1 206 206\n",
      "1 207 207\n",
      "1 208 208\n",
      "1 209 209\n",
      "1 210 210\n",
      "1 211 211\n",
      "1 212 212\n",
      "1 213 213\n",
      "1 214 214\n",
      "1 215 215\n",
      "1 216 216\n",
      "1 217 217\n",
      "1 218 218\n",
      "1 219 219\n",
      "1 220 220\n",
      "1 221 221\n",
      "1 222 222\n",
      "1 223 223\n",
      "1 224 224\n",
      "1 225 225\n",
      "1 226 226\n",
      "1 227 227\n",
      "1 228 228\n",
      "1 229 229\n",
      "1 230 230\n",
      "1 231 231\n",
      "1 232 232\n",
      "1 233 233\n",
      "1 234 234\n",
      "1 235 235\n",
      "1 236 236\n",
      "1 237 237\n",
      "1 238 238\n",
      "1 239 239\n",
      "1 240 240\n",
      "1 241 241\n",
      "1 242 242\n",
      "1 243 243\n",
      "1 244 244\n",
      "1 245 245\n",
      "1 246 246\n",
      "1 247 247\n",
      "1 248 248\n",
      "1 249 249\n",
      "1 250 250\n",
      "1 251 251\n",
      "1 252 252\n",
      "1 253 253\n",
      "1 254 254\n",
      "1 255 255\n",
      "Computing truncation loss took 207.0 sec.\n",
      "Saved initial and final model to '../data/sst_lstm_nlayers_2_nhid_256_emb_pretrained_fix_dim_100_dropout_05_weight_decay_0000_seed_4023.pt'\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Evaluate truncated networks\n",
    "max_rank = HIDDEN_DIM\n",
    "# rank_step = HIDDEN_DIM // 128\n",
    "rank_step = 1\n",
    "trunc_ranks = np.arange(0, max_rank, rank_step)\n",
    "n_rank = len(trunc_ranks)\n",
    "\n",
    "# Keys\n",
    "keys_rnn = model.keys_rnn\n",
    "n_states = len(keys_rnn)\n",
    "nhid = HIDDEN_DIM \n",
    "\n",
    "svd_blockwise = False\n",
    "\n",
    "############################################################################\n",
    "trunc_loss_acc = np.zeros((2, n_rank, 2))\n",
    "\n",
    "time0 = time.time()\n",
    "for j in range(2):\n",
    "    trunc_dw = j == 0\n",
    "    for i, rank in enumerate(trunc_ranks):\n",
    "        print(j, i, rank)\n",
    "\n",
    "        # Weights and biases for truncated model\n",
    "        state_dict_trunc = OrderedDict()\n",
    "        \n",
    "        # Weights\n",
    "        for key in state_dict_final.keys():\n",
    "            # Truncate only 'inner' weights with NxN\n",
    "            cond_trunc = (\n",
    "                ('rnn.weight' in key) \n",
    "                * (('hh' in key) or ('ih' in key))\n",
    "                * (not 'ih_l0' in key))\n",
    "            if cond_trunc:\n",
    "                # print(\"Truncate\", key)\n",
    "                if svd_blockwise:\n",
    "                    # LSTM and GRU cells have combined weights for the effect of the\n",
    "                    # last hidden state on the new state and the gates. \n",
    "                    w_trunc_rnn = np.zeros((n_states * nhid, nhid))\n",
    "                    for k, key_rnn in enumerate(keys_rnn):\n",
    "                        # Get weights\n",
    "                        w = state_dict_final[key][k*nhid : (k+1)*nhid]\n",
    "                        # Move to cpu\n",
    "                        w = w.cpu()\n",
    "                        if trunc_dw:\n",
    "                            # Obtain changes\n",
    "                            w0 = state_dict_init[key][k*nhid : (k+1)*nhid]\n",
    "                            w0 = w0.cpu()\n",
    "                            dw = w - w0\n",
    "                            # Truncate dw at given rank:\n",
    "                            u, s, vT = np.linalg.svd(dw, full_matrices=False)\n",
    "                            dw_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                            # Unite with initial connecitivity\n",
    "                            w_trunc_rnn[k*nhid : (k+1)*nhid] = w0 + dw_trunc\n",
    "                        else:\n",
    "                            # Truncate w at given rank:\n",
    "                            u, s, vT = np.linalg.svd(w, full_matrices=False)\n",
    "                            w_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                            w_trunc_rnn[k*nhid : (k+1)*nhid] = w_trunc\n",
    "                    # Add to state dict\n",
    "                    w_trunc_rnn = torch.from_numpy(w_trunc_rnn).to(device)\n",
    "                    state_dict_trunc[key] = w_trunc_rnn\n",
    "                else:\n",
    "                    # Get weights\n",
    "                    w = state_dict_final[key]\n",
    "                    # Move to cpu\n",
    "                    w = w.cpu()\n",
    "                    if trunc_dw:\n",
    "                        # Obtain changes\n",
    "                        w0 = state_dict_init[key]\n",
    "                        w0 = w0.cpu()\n",
    "                        dw = w - w0\n",
    "                        # Truncate dw at given rank:\n",
    "                        u, s, vT = np.linalg.svd(dw, full_matrices=False)\n",
    "                        dw_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                        # Unite with initial connecitivity\n",
    "                        w_trunc_rnn = w0.numpy() + dw_trunc\n",
    "                    else:\n",
    "                        # Truncate w at given rank:\n",
    "                        u, s, vT = np.linalg.svd(w, full_matrices=False)\n",
    "                        w_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                        w_trunc_rnn = w_trunc\n",
    "                # Add to state dict\n",
    "                w_trunc_rnn = torch.from_numpy(w_trunc_rnn).to(device)\n",
    "                state_dict_trunc[key] = w_trunc_rnn\n",
    "            else:\n",
    "                # print(\"Leave\", key)\n",
    "                state_dict_trunc[key] = copy.deepcopy(state_dict_final[key])\n",
    "        \n",
    "        # Define truncated model\n",
    "        model_trunc = RNN(RNN_TYPE, INPUT_DIM, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                          N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, TRAIN_EMB)\n",
    "        model_trunc.load_state_dict(state_dict_trunc)\n",
    "        # Evaluate\n",
    "        model_trunc.to(device)\n",
    "        trunc_loss_acc[j, i] = evaluate(model_trunc, valid_iterator, criterion)\n",
    "        del model_trunc\n",
    "print('Computing truncation loss took %.1f sec.' % (time.time() - time0))\n",
    "\n",
    "\n",
    "# Save everything\n",
    "state_dict_final = model.state_dict()\n",
    "with open(SAVE, 'wb') as f:\n",
    "    torch.save({'state_dict_init': state_dict_init,\n",
    "                'state_dict_final': state_dict_final,\n",
    "                'loss_acc': loss_acc,\n",
    "                'trunc_loss_acc': trunc_loss_acc,\n",
    "                'trunc_ranks': trunc_ranks,\n",
    "                'svd_blockwise': svd_blockwise,\n",
    "                }, f)\n",
    "# print(\"Saved last model to '%s'\" % SAVE)\n",
    "print(\"Saved initial and final model to '%s'\" % SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-wfE8bTaidk"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAFzCAYAAAAkFp78AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABjrElEQVR4nO3de3hU9Z348ffcMjNJJsnkTi6QhEAICRAgARTFCyLWVVoVLZVudamiK7Vb29W13S1qf91q69balnVbtNuoy0pRq6i4iFUQERXDReQiCbnf7/e5z5zfH2GODBkIl9xIPq/n4Xkyc87M+Z6Z4XzO9/b5ahRFURBCCCFOoR3pAgghhBidJEAIIYQISgKEEEKIoCRACCGECEoChBBCiKAkQAghhAhKP9IFGCyxsbGkpaWNdDGEEOKiUlFRQUtLS9BtYyZApKWlUVRUNNLFEEKIi0p+fv5pt0kTkxBCiKAkQAghhAhKAoQQQoigxkwfxGBwuVz4fD5MJtNIF0WIMcntdlNTU4PD4Rjpoow7JpOJlJQUDAbDWb9GAsRJvvjiCwDmzp07wiURYmyqqanBYrGQlpaGRqMZ6eKMG4qi0NraSk1NDenp6Wf9OmliEkIMG4fDQUxMjASHYabRaIiJiTnnmpsECCHEsJLgMDLO53OXABGEz+cb6SIIIYZAa2sreXl55OXlkZiYSHJysvrY5XINSxk6Ojp45plnLug9CgsL+d73vjdIJTo9CRBBeL3ekS6CEGIIxMTEcODAAQ4cOMC9997LAw88oD4OCQnB4/EMeRkGI0AMFwkQQQzHj0QIMTrceeed3HvvvcyfP5+HHnqIRx99lP/4j/9Qt+fm5lJRUUFFRQXZ2dncfffd5OTkcO2112K32wE4fvw411xzDbNmzWLOnDmUlpbS09PD4sWLmTNnDjNmzGDz5s0APPzww5SWlpKXl8eDDz4IwJNPPklBQQEzZ87kkUceCVrOP//5z0ydOpV58+bx0UcfAX03s+np6SiKQkdHBzqdjp07dwKwaNEiSkpKLuizkVFMQUiAEGLoPfvss5SXlw/qe6anp3P33Xef8+tqamrYvXs3Op2ORx999LT7lZSU8NJLL/Hss89y22238eqrr/Ltb3+blStX8vDDD3PTTTfhcDjw+XyEhITw2muvERERQUtLCwsWLGDZsmU88cQTHDp0iAMHDgCwbds2SkpK2LNnD4qisGzZMnbu3MmiRYvU49bX1/PII4+wd+9eIiMjueqqq5g9ezY6nY6srCyOHDlCeXk5c+bM4cMPP2T+/PlUV1czZcqUc/4sTiYBIghpYhJifLn11lvR6XQD7peenk5eXh7QNxy+oqKC7u5uamtruemmmwDUeVRut5uf/OQn7Ny5E61WS21tLY2Njf3ec9u2bWzbto3Zs2cD0NPTQ0lJSUCA+PTTT7nyyiuJi4sD4Jvf/CbFxcUAXH755ezcuZPy8nJ+/OMf8+yzz3LFFVdQUFBw/h/ICRIgTuLxeNDr9VKDEGIYnM+d/lAJCwtT/9br9QEDVU4eGmo0GtW/dTqd2sQUzIYNG2hubmbv3r0YDAbS0tKCDjNVFIUf//jH3HPPPedV9kWLFvFf//Vf1NXV8bOf/Ywnn3ySHTt2cPnll5/X+51M+iBO4h/FIDUIIcavtLQ09u3bB8C+ffsGbAazWCykpKTw+uuvA+B0OrHZbHR2dhIfH4/BYGD79u1UVlaq+3d3d6uvX7p0Kf/93/9NT08PALW1tTQ1NQUcY/78+XzwwQe0trbidrt5+eWX1W3z5s1j9+7daLVaTCYTeXl5/PGPfwyogZwvCRAn8QcGqUEIMX7dcssttLW1kZOTw7p165g6deqAr3nxxRf53e9+x8yZM7n00ktpaGhg5cqVFBUVMWPGDF544QWmTZsG9I2kWrhwIbm5uTz44INce+213H777VxyySXMmDGD5cuXBwQQgAkTJvDoo49yySWXsHDhQrKzs9VtRqOR1NRUFixYAPQ1OXV3dzNjxowL/iw0iqIoF/wuo0B+fv4FrwexY8cOLBYLsbGxTJo0aZBKJoTwO3r0aMDFTQyvYJ//ma6dUoM4iX+modQghBBCAkQAf4AYrhmVQggxmkmAOInUIIQQ4iujephrWloaFosFnU6HXq8f8jWnJUAIIcRXRnWAANi+fTuxsbHDcix/gJBkfUIIIU1MpyVzIYQQ492oDhAajYZrr72WuXPnsn79+iE/nlarpb29HUCWRBRiDBor6b6Hy6huYtq1axfJyck0NTWxZMkSpk2bFjA7cP369WrgaG5uvuDjaTQaGhsbsVqt2O32gOn3QoiLnz/dN8Cjjz5KeHg4//zP/6xu96fbGUr+AHHfffcN6XEGw6iuQSQnJwMQHx/PTTfdxJ49ewK2r169mqKiIoqKitQkVhdCo9HQ1taGoihnzLEihBg7LoZ03y+//DI//OEPAfjtb39LRkYGAGVlZSxcuHDIPptRW4Po7e3F5/NhsVjo7e1l27ZtrF27dkiPqdFo8Hq9uFwuaWISYohVV1djs9kG9T1DQ0NJTU0959eN9nTfl19+Ob/61a8A+PDDD4mJiaG2tpYPP/xwUHIunc6oDRCNjY1q+lyPx8Ptt9/OddddN6TH1Ol0+Hw+HA6H1CCEGEdGe7rvxMREenp66O7uprq6mttvv52dO3fy4YcfcvPNN1/o6Z/WqA0QGRkZfP7558N2PH9KKkVR6OnpITIycljaI4UYr87nTn+oXAzpvi+99FL+/Oc/k5WVxeWXX85///d/8/HHH/PrX//6rM7xfIzqPoiR4PP51JFMXV1dI1waIcRwG43pvqGvmek//uM/WLRoEbNnz2b79u0YjUYiIyMH47SDkgBxgr8G4fP5qK+vx2QyUV9fzxhJdiuEOEujMd039AWI6upqFi1ahE6nIzU1lcsuu2zQz/9kku77BK/Xy4EDB9i2bRsOh4N/+qd/ory8nMmTJxMVFTV4BRViHJN03yNL0n2fp5P7IOx2O1arFY1Go1b7hBBivJEAcQqfz4fdbkej0RAaGkpvb+9IF0kIIUaEBIgTTu6D8I9MCA0NxWazST+EEGJckgBxwslNTP6haGFhYeq8CCGEGG8kQJxwuhoEMOizPYUQ4mIgAeIEf3pvvV6P2+3GZrNhMpnQarXSDyGEGJckQJzgDxDh4eEAtLW1odFoMJlM0sQkxBji/z9+JnfddRdHjhwB4Be/+EXAtksvvXRQjnExkABxgn9qvcViAaClpQUAs9ksAUKIcea5555j+vTpQP8AsXv37pEo0oiQAHHCqQGitbUV6Eu85Xa7ZZ1qIcaYHTt2cOWVV7J8+XKmTZvGypUr1b7IK6+8kqKiIh5++GHsdjt5eXmsXLkS+Kp2cLp03mOJZKI7wd/EFBERAXxVg/BnZnQ4HGOm2ijEaFFYWNjvuZycHAoKCnC73WzYsKHfdv8KcDabjU2bNgVsu/POO8/p+Pv37+fw4cMkJSWxcOFCPvroo4D0FU888QTr1q1TU3OfzGQyBU3n7V/bfiyQGsQJ/gBhMBiIjIxUaxBmsxmQJUiFGIvmzZtHSkoKWq2WvLw8Kioqzvq1iqLwk5/8hJkzZ3LNNdecNp33xUxqECf4m5g0Gg0xMTFqDSIkJASNRiMBQoghcKY7foPBcMbtoaGh51xjONWp6bvPpSn5bNN5X8ykBnGCP0BotVpiYmLUGoR/JJMsICTE+GQwGHC73f2eP10677FEAsQJp6tBQN9dhsvlGqmiCSFG0OrVq5k5c6baSe13unTeY4k0MZ3g74PQarXExsbS3d2N0+nEaDQSEhIiiwcJMUb4MzRfeeWVXHnllerz69atU//esWOH+vcvf/lLfvnLX/Z7fWxsLB9//PEZj3GxG/U1CK/Xy+zZs7nhhhuG9Din1iAAmpubgb4qps/nU4OIEEKMB6M+QPz2t78dlgVGTu6DyM7ORqfTqUsIhoSEAEgzkxBiXBnVAaKmpoYtW7Zw1113DfmxTg4QSUlJ3HDDDbz77ruUlZWpASJYR5UQQoxVozpA/OAHP+BXv/oVWu3QF/PkJiaAW2+9FUVR2LdvHwaDAZAahBBifBm1AeKtt94iPj6euXPnnnaf9evXk5+fT35+vtpfcL5OrkFA34zqmJgYqqurAwKEDHcVQowXozZAfPTRR7zxxhukpaWxYsUK3n//fb797W8H7LN69WqKioooKioiLi7ugo7nz8Gi0+nU51JSUqipqUGr1aLX62lqauLIkSNjbjKMEEIEM2oDxOOPP05NTQ0VFRVs3LiRq6++mv/5n/8ZsuP5RyidnEclNTWVmpoaFEUhJCRE3cfpdA5ZOYQQQ0un06n5nPzpNc4mhffp3HnnnbzyyiuDWMLRQ+ZBnOCvQZzc35GSkoLdbqelpUVtZgLpixDiYmY2m/sl3xtPKbzPxaitQZzsyiuv5K233hrSYwRrYkpNTQX6RlP5RzKBBAghxhp/puYzpQD/2c9+RkFBAbm5uaxevVp9fiyTGsQJp45igr4aBEB1dTXTpk3DbDbT0NAgAUKIQTIS6b796zsApKen89prrwVsP10K8O9973usXbsWgL//+7/nrbfe4sYbbzy7E71ISYA4IVgTU1RUFOHh4dTW1mI2mzGbzbS1tUmAEOIiFqyJ6WT+FOCA2kdx2WWXsX37dn71q19hs9loa2sjJydHAsR44fP50Gq1AU1MGo2GhIQEmpqa1OdCQkLGTJ4VIUbaSKf7DiZYCnCHw8F9991HUVERqampPProo+NiNONF0QcxHILVIADi4+P7BQiXyzUu2h+FEH38wSA2Npaenp4xO2rpVFKDOOFMAWLv3r0oioJGowlIu3Fyx7UQYuyKiori7rvvJjc3l8TERAoKCka6SMNCAsQJwUYxASQkJOByuejs7CQqKkoNChUVFZjNZpKSkvq9RggxegVrIj6bFOA///nP+fnPf97vtcE62scKaWI6wT+K6dSLfXx8PIDazORvn6yoqKCpqYk33niD7du3D2NJhRBieEiAOOFMTUzwVYAwmUxMmDBBbZOMiYnhj3/84zCWVAghhsewBIje3l71Dr24uJg33nhj1KXOPl0Tkz9ANDY2Buz73HPP4XK5MBqNOBwO6bQWQow5wxIgFi1ahMPhoLa2lmuvvZYXX3xxSIanXYjT1SBCQ0OxWCwBI5na29uBvuYmvb6vG0fmRghxduRmamScz+c+LAFCURRCQ0P561//yn333cfLL7/M4cOHh+PQZ01RFHw+X9AO5/j4eOrq6vD5fNTV1dHR0QH0BQ+AsLAwSeAnxFkwmUy0trZKkBhmiqLQ2tqKyWQ6p9cNyygmRVH4+OOP2bBhA3/6058ARuX6zqcLEHPnzmXTpk38+Mc/5ujRo+rsSYvFQm9vL+Hh4TgcDiIiIoa7yEJcVPwp9C90/RZx7kwmkzpD/GwNS4B4+umnefzxx7npppvIycmhrKyMq666ajgOfdYURUFRlKCr133zm99k7969HD16FIDPP/8crVaLxWKhoaGBsLCwcTGrUogLZTAYSE9PH+liiLM0LAHiiiuu4IorrgD67tJjY2P53e9+NxyHPmtnamIyGAw88sgjFBcX8/Of/5yqqiqsVqs65NVfgxBCiLFkWPogbr/9drq6uujt7SU3N5fp06fz5JNPDsehz4k/H1MwUVFRzJs3D6vVqj72rxEhAUIIMRYNS4A4cuQIERERvP7663zta1+jvLycF198cTgOfdbO1MR0sqSkJKAvQPj3lSYmIcRYNCwBwu1243a7ef3111m2bBkGgyFg3YXR4ExNTCdLTk4G+gIE9M2bkBqEEGIsGpYAcc8995CWlkZvby+LFi2isrJyVI74OVMTk9/JNQgAvV4vNQghxJg0LAHi+9//PrW1tbz99ttoNBomTZo0YP4ih8PBvHnzmDVrFjk5OTzyyCNDWkZ/DWKgAHFqDcJgMEgNQggxJg1LgOjs7OSHP/wh+fn55Ofn86Mf/Yje3t4zvsZoNPL+++/z+eefc+DAAbZu3conn3wypOU8m8k7kyZNQqvVMmHCBLWcEiCEEGPRsASIVatWYbFY2LRpE5s2bSIiIoJ/+Id/OONrNBqNupC4vw9jqPstziZAJCYm8swzzzBv3jygrwZhNBplJrUQYswZlnkQpaWlvPrqq+rjRx55RF00/Ey8Xi9z587l+PHjrFmzhvnz5w9hKc8+V4m/HwL6Oqm1Wq3kYhJCjDnDUoMwm83s2rVLffzRRx9hNpsHfJ1Op+PAgQPU1NSwZ88eDh06FLB9/fr1arPVhU7d9/dBnCv/qCePx3NBxxdCiNFmWGoQf/jDH/jOd75DZ2cnAFarleeff/6sXx8VFcVVV13F1q1byc3NVZ9fvXo1q1evBiA/P/+Cy3k+CcT8AWI05pYSQogLMSw1iFmzZvH5559z8OBBDh48yP79+3n//ffP+Jrm5mY1a6rdbufdd99l2rRpQ1ZGjUZzXgHCn+77fGofQggxmg3rinIRERHq/IennnrqjPvW19dz1VVXMXPmTAoKCliyZAk33HDDkJXNP5P6XPlrEJK+WAgx1gxLE1MwA11QZ86cyf79+4epNOdfgxho5rUQQlysRmxN6tGWauN8SYAQQoxVQ1qDsFgsQQOBoijY7fahPPR5uZBRTAPNwBZCiIvNkAaI7u7uoXz7UUGj0ZxVkj8hhLjYyG3vSc6nD0IChBBirJIAccL5dlJDX9OUwWCQuRBCiDFFAsQJF9JprigKJpNJ8jEJIcYUCRAnuZC5DEajUc3oer5zKoQQYjSRAHHChdQgtFotJpMJm82Gx+PhyJEjVFRUDF7hhBBiBIzYRLnR6Hzv+g0GAyaTie7ubrq7u3E4HJJ6Qwhx0ZMaxAkXUoMwGAyYzWZ6e3ux2WxYLBZcLpdkeBVCXNQkQJxwIQHCaDRiMBjQaDRMnDiRxMREgFE5GVAIIc6WBIgTLjRAQN/Kd7GxsepaFzabbVDKJoQQI0ECxAlarfa8+yD8S6PW19ej0WgwGAzo9XoJEEKIi5oEiBMuJEDExMTw7LPP0tjYqD4XGhpKd3c3XV1dg1VEIYQYVhIgTtDpdOcdIPzNUyfnnoqLi8Pn81FSUkJra+uglFEIIYaTBAj6hrdeSICAvmamkwNEVFQUM2fOxGKxUFlZqa6OJ4QQFwsJEHw1/+FCAkRERAQ9PT0Bz2m1WjIyMjCbzZSWllJWVtZvHyGEGK0kQDA4y4WGh4cH7W/Q6/VMnTqV2NhYOjs7qauru+BjCSHEcBi1AaK6upqrrrqK6dOnk5OTw29/+9shO9ZgBAiLxXLa2oFOp2PSpElERUWp+ZqEEGK0G7WpNvR6Pb/+9a+ZM2cO3d3dzJ07lyVLljB9+vRBP5Y/LcaFBAqLxaLmYtLrg3+sJpOJtrY2vF6vrB8hhBj1Rm0NYsKECcyZMwfou/hmZ2dTW1s7JMcarBoEcMY+BpPJBCBpwYUQF4VRGyBOVlFRwf79+5k/f/6QvL8/QFzIXb1/styZlln1z7iWZiYhxMVg1AeInp4ebrnlFp5++mkiIiICtq1fv578/Hzy8/Npbm4+72P4m5i02vP/OPxlO1OA8NcgRmuAUBSF7u5uSTIohABGcR8E9OU2uuWWW1i5ciU333xzv+2rV69m9erVAOTn55/3cfwBYjBqEGeaOa3VagkJCTljgLDZbDQ2NqLT6QgJCcFqtao1j6Hg8/lQFAWfz0dlZSWdnZ1AXxPfhAkTLihHlRDi4jZqA4SiKHz3u98lOzubH/7wh0N6LP8d84UEiOTkZDQaDRUVFSxYsOC0+5lMJrq6uigtLSU+Pp7Q0FBKSkrweDwYjUa6u7vRaDRoNBq8Xi9NTU1kZmai1WrR6XQYDAY8Hg86nU69eHs8HtxuNyEhIWd1Dg6Hg7KyMvV4J6+lnZSUhN1up76+nq6uLhISErBaref9uQghLl6jNkB89NFHvPjii8yYMYO8vDwAfvGLX3D99dcP+rFcLhfAaUcfnY3Q0FBSUlIoLi4+436xsbH4fD56enro6OhAp9Ph9XqxWCy43W6io6NJTk7GYDBgs9koLi7m6NGj6uvNZjN2u53IyEgmT55MXV0dDQ0N6vawsDBMJhM6nY7IyEigrwPdH0wURaGqqgqn04nH48FisRAaGopGoyEyMhKz2YyiKISHh9PU1ERZWRlxcXFMnDjxvD8bIcTFadQGiMsuu2zY1nV2u93AhQUIgKlTp1JUVISiKKdtmrFarVitVnw+Hy0tLTQ3N5OSkkJsbGy/fUNDQ5k2bRrd3d1otVpcLhcdHR1ERETQ2dnJ4cOHcTqdREdHExkZid1up6uri66uLjweD01NTUBfgPDnhurs7KS7u5uJEycSFxcXtIwajYb4+Hji4uKoqqqiubmZxMREQkJCLujzEUJcXEZtgBhOgxUgpkyZwnvvvUdTUxMJCQln3Fer1RIfH098fPwZ9zOZTGrnNvT1DSiKQllZGXa7nYkTJxIbG6sGpOTkZAC8Xi+9vb04HA5qamrUznOtVktiYmLQgHQqjUZDdHQ0LS0t2O12CRBCjDMSIBicPgiArKwsAEpKSgYMEBdCo9EwefLkM+6j0+mIiIggIiICq9WKx+NBq9Wi1+vP6Tz9ix/5m7WGi6IoKIoy4Mgyf5OZwWBgwoQJ+Hw+3G43RqNROtiFuEASIPiqBnGhd8iTJk0iJCSEQ4cOcdlllw1G0QaFwWDAYDCc12v1ej0hISEXvHzqyRd8n89Hb28viqJgMpn6fe4dHR3U1NTgcrmwWCxotVpMJpO6EJO/TEajkba2NlpaWgBobGxUR6SFh4eTnp4utR4hLoAECL6qQZzvRdRPr9eTl5c3YD/ExcZsNg+4Op6/v8jj8eDz+QgJCUGj0ahNXP7hvyaTCYfDEdC/FB4eTnh4ODExMbS0tNDY2IjZbCYmJkYNJMHSpfs/37CwMBISEujp6UGn06HVaqmvr6eqqorMzMxB+hSEGH8kQDB4AQJg3rx57Nmzh4qKCtLT0y/4/UYDs9lMZ2cnPp9P7Sx3Op34fD48Hg9Op5Pm5uaACXYajYaQkBCcTqfa3wJ98zzi4+MJDw9Hp9PR3d1NZ2cnDQ0N6mis2NhYUlNTA5qXFEXB4/Go/1wuF3a7Ha/XS0JCAiaTqd9w3NraWjo7Owe1aUxRFHUEmNfrxev1Yrfb1aHHISEhxMbGXtCkSyFGCwkQfDVRbjCaIwoKCtBoNOzZs2dMBQiA48eP4/V6g9YmLBaLetHX6XTY7XacTiexsbHExMScNvhaLBaSkpJwOp20t7cTERFBaGhov/38a32fbRCPj4+npaWF8vJyMjIy0Ov1KIqCy+VCq9Wqx+js7MThcBAZGUl4ePhpa31utxu73U5DQ0PQ2fJ6vR6v16vORs/IyBgzNUgxfkmAADUD62AECKvVyvTp03nrrbdYunQpUVFRF17AERYZGUl8fDxdXV0YDAaSkpIICwtTO739/y6E0WgkMTFxkErcN1prypQpfPnll5SUlAy4f2NjI3FxcXg8Hux2O1qtFq1WS2RkJA6Hg7a2NrXZMCUlRZ1r4q81+Dv+m5qaqK6u5vjx46Snp6PVamlvb6e7u1udzOhf/zwkJASLxaLWjnQ6HXq9Xu1rEWKkya+QviGh/v+Yg+Gee+7hRz/6Ef/6r/9KcnIyJpOJ7OxsrrrqqoAhqxcLnU5HamrqSBfjnBmNRrKzs+nt7QW+avby+XxqLSAyMhKj0UhdXR1NTU1oNBoiIiJQFAW3201tbS1arZaYmBisVmvQTvWTxcfHo9FoqK6u5ujRo2ptyn8D4u9T8c+UP53w8HAURcFisRASEkJoaChut5vGxkb0ej0ajYbw8HCioqIwGAxSWxFDQgIEfU1MXq930HIepaWl8f3vf5833niD+vp6uru72bFjBx9//DFf//rXcTqdXHrppYNyLHFmISEhQS/o/txZfikpKYSFhWE2m9UmNX+QONcLcFxcHKGhoZSWluJyuZg8eTKRkZH93sPlctHb26vWRLxeLx6PB5vNpqZcOXmWPPQFPf9AgPb2dqqrqwkNDSUrK0vt9/APAJCgIS6UBAi+ChCDOSTyiiuu4IorrgD6/sO+8847PPPMMxw4cACNRsPPf/5zZsyYcd7vf+jQIbxeL7NmzRqsIo9r/kmBpz53vr+JsLAwcnJyUBTltM1FpwteJ5fDHwyam5txu92kpqai0+lQFAWbzUZXVxd1dXXU1NQQGRlJU1OTuiaJ2WzGaDSi1+uJi4vDZDLh8/nw+XwBubxAgooITgIEqKNxhmrMvEaj4brrrsNut6MoCu+++y6//vWveeihh7Db7WRnZwftmAWoq6vjgw8+oLq6Wp1HEBUVxZYtW9Dr9Tz11FOSJ2mUGoxVA/3BxT9D3k+j0RAWFkZYWBgul4vm5maam5sxGAzExMQAfZMbe3p6cLvdNDU1qXNQ/GVLSUlRa0ktLS34fD51QIE/gGg0Gkwmk/r79AcXMT5olOFKeDTE8vPzKSoqOq/Xbt++HY1Gw8KFCwetH+JMysvLeeyxx2hrawNgwYIF/PjHP1bv3txuNx0dHXz66acUFhbidrtJTExEr9fjdrtpaGggJyeHmpoadDod6enp6jDTpKQk8vPzueyyywKS9Imxyz9yyt9nceoQW7fbTXNzs9rXptPpaG1tDRiNFhYWhl6vV9O9n8o/Cszr9TJhwgSSkpKG9JzE8DnTtVMCBPDee+8BcPXVVw/bBbWrq4sPPviAhoYG3nzzTebMmUNXVxe1tbUBs5Zzc3P50Y9+pN4VQt9cArPZzJEjR9i0aROdnZ3ExcVhMBgoKyujrq4O6FvE6PrrryciIgKLxYLX6+WLL76grq6O66+/HqPRiMPhwGg0YrVaiYqKwmg0UlZWhsPhIDw8XF0IyePx0NDQwL59++js7CQlJYWvf/3rhIeHqyN+RvLO0r+mhX/G9sn/fD4fDoeD9vZ2oC+lub+f4Uy8Xi9Op1MdXXS68/OPQnI6nTidTnXAw8kZdIPx36H79wv296n7DBZ/RmF/E5T/3Pyfl3/Irj/49Pb2otFo8Hg8dHR0YLFYCAsLU1+n0WgwGo1YLJZB+R34fL4Bz9vr9dLZ2YnL5cJms2E0GtW+Jf85+Ofu+Pt5tFotbrdb7Vvy/wsJCcHr9arb/N95MP7Re8HKZrfb1SzN/uY8/2fpT9cfEhKi/kYMBsOIz5kZFwEiPT2dRx55JOC5nJwcCgoKcLvdbNiwod9r8vLyyMvLY+vWrRw9erTfRKv8/Hxyc3Pp7Ozktdde6/f6Sy65hKysLFpaWnjrrbf6bV+0aBEZGRk0NDSwdevWftsXL15MUlISTz31FB0dHej1enX4pF6v5/LLLyc/P5/y8nJ27tzZ7/U33HADsbGxHDt2jI8//lh93m6309vbS29vL3v37sVqtaqZW/3/UVwuF6WlpXi9XmJiYgICkF9JSQmKohAXF6d+Nv5hnXa7XU1tnpCQQGRkpDq/IDo6Gp1Ox9e+9jW2bdtGc3OzepH0j+AJDw9n1qxZxMfHU19fT3V1Nb29vWoTiNFoZPr06fh8Po4dO0Z3d3fAyn+hoaFMnToVRVHYvXu3OmnOz2azUVNTA/QNGji1+dDhcKjZbv3t+n6KotDV1UV9fT2Auh4HfNVG39XVRWtrq/r+p2pvb6e5uRmNRsOUKVP6bW9tbaW1tRWdThc0r1ZzczPt7e0YDIaA+TT+i2Z7ezu9vb0YDIagw4P9NQSj0Rg0IWRLSwsOhwOTyRQ0cWNzczNOp1P9Pk9msViYMWMGycnJNDU1UVpaGvDZuVwusrKyiI6O5vDhw3R0dKjNW16vF41GQ2JiIkajkZ6eHjo7O/sFw0svvRSdTkdJSQn19fXqRdZ/Yc7OzsZisVBTU0NjYyNer1f9DnU6nbo8cXFxsZqKxS8kJERdYOzo0aPqjYOf2Wxm9uzZABw+fLhfrSo8PJyZM2ficDg4cuRIQE3MPzQ6JycHgH379qlNy/7fkNVqJTs7G4CioiJ1u/9faGgosbGxeL1eqqur1RsfP5PJREREBHq9noaGBvU36f8/frbXPZvNxqZNm1i3bt1pA4T0QfBVnqCRoNPpWLFihVqLOVlqaup53Tn6R+LcfffdeDwejh8/zv79+wM6XXt7e7njjjvU/8THjh1TZwebzWZ0Oh233347DoeD6upqGhsbMRgMmEwmNBoNTqeThQsX4vV6aW5uVhce6ujooKqqCp/Pxw9+8AMMBgOZmZkYDAY1OCiKQltbGzt27AD67uhPHVXkcrn4v//7P6BvhNGpfTQOh4N33nkH6EuSaLVa1Ts+f7v54sWL1SGnHo9HbT50OBzo9XrmzJkDoLa/n/rZX3XVVWqN6uQ7an+Z/LUrf/u+VqtVzy8rK0td+6O8vLzfd5SdnY3VasXtdlNVVdVve1ZWFhaLBZfLpY5kOvn4qamp6oimYM1CKSkpGI1G3G43XV1d/X7fKSkphISE4HK5gk78S0lJwWAw4HQ61WHCJ6uurqa+vl4diQV9F0f/HA6tVkt3dzcxMTEBF0j/55OWloZer6e2tlZ9vb+MPp9PHV7sX33RHxj8d+VarZbOzk4aGxvV8vuDi1ar5eDBg/h8Pjo6OnC5XAG1EYfDwb59+wgJCVG3+wOYf6b8l19+iV6vp6Ojo98KkD6fj6qqKvR6vTqr3k9RFHUypX849cm/LY1Gg8vlUn+PNpstoLbkv/lJSEhAq9UG/W3Gx8eTkZGhjmQ7+bMbbGOmBnEhTUx/+9vfcLlcQ7IY0Xhjs9morKxUs6zOnTs36LoTnZ2d6r7l5eUoisLs2bOxWCwB+2m1WsxmM6GhoWpw86f6cDqduFwuEhMTpeNUiPN0pmun1CBOGCNxcsSFhoaq1efp06efdr/IyEgiIyOZMGHCGZdoDcZoNA7pOt1CiD6SUewECRBCCBFo1AaIVatWER8fT25u7pAfy98uKoQQ4iujNkDceeedQUf+DBUJEEIIEWjUBohFixb1G143VEZ6HLIQQoxGF3Un9fr161m/fj3QN277fMlsYyGE6O+ivnVevXo1RUVFFBUVBR1KebakBiGEEP3JlRHUCTxCCCG+IgGCwcm6KYQQY82oDRDf+ta3uOSSSzh27BgpKSn86U9/GrJj+dMkCCGE+Mqo7aR+6aWXhuU4JycAE0II8ZVxf9t8coZQIYQQXxn3V0WXywVIgBBCiFON2iamc9Xa2kphYWHAc2eTFz0tLQ2Xy8Xhw4fVhXb8hmM9iNTUVKqrq4Om+77uuutITEykrKzsnNaD8LvpppuIjIzk0KFDQbM13nbbbYSGhnLgwAEOHDjQb/vKlSsxGAx89tlnHD58uN/2O++8E4Ddu3era0P4GQwGVq5cCcAHH3zQL+V1aGgot912G9CXTde/doNfREQEN998MwBbt25VU177xcTEcOONNwLw5ptvqmsz+CUmJnLdddcB8Ne//pWurq6A7SkpKVxzzTUAbNq0KSCnP/StL+JfU3zDhg243e6A7VOnTuXSSy8F6Pe7g3PPyX8q+e3Jbw9G5rd3snF/26woirpWghBCiK/IehBCCDGOnenaOe5rEEIIIYKTACGEECIoCRBCCCGCkgAhhBAiKAkQQgghghozo5hiY2NJS0s779c3NzdfUMrwi8l4OleQ8x3LxtO5wtCcb0VFBS0tLUG3jZkAcaHG0zDZ8XSuIOc7lo2nc4XhP19pYhJCCBGUBAghhBBBSYA4YfXq1SNdhGEzns4V5HzHsvF0rjD85yt9EEIIIYKSGoQQQoigJEAIIYQISgKEEEKIoCRACCGECEoChBBCiKAkQAghhAhKAoQQQoigJEAIIYQISgKEEEKIoCRACCGECEoChBBCiKAkQAghhAhKAoQQQoigJEAIIYQISgKEEEKIoCRACCGECEoChBBCiKAkQAghhAhKAoQQQoigJEAIIYQISgKEEEKIoCRACCGECEoChBBCiKAkQAghhAhKP9IFGCyxsbGkpaWNdDGEEOKiUlFRQUtLS9BtYyZApKWlUVRUNNLFEEKIi0p+fv5pt0kTkxBCiKAkQAghhAhKAoQQQoigJEAIIS4a7e3t9PT0jHQxxo0x00ktzs+f/vQnPv/8c3Jzc7n77rvRaDQjXSQxxrlcLlpbW4mPj8fhcGC32wEwGAxEREQE/Q0qikJtbS2NjY0A6HQ6QkNDmTp16rCWfbyRADGG9fb28uc//5mlS5cyYcIE9Ho9ISEhNDQ0EBcXR01NDZs3byYpKYm33nqLlJQUrFYr8+bNQ6fTjXTxxx2v1wugfvY+nw+tVovH46G3txeAkJAQzGbziJVxICefg8PhoL6+nsTERLXMdrud48eP43K56OzsVM/Lz2q1MmnSJLRaLTU1NSiKQmpqKs3NzTQ2NhIXF4fJZKK9vZ3u7m7cbjcGg2HYz3O8kAAxhm3ZsoVt27bxwQcf4PP5SE9PZ86cOWzcuJGQkBBiY2Mxm8386le/4tFHH+UPf/gDAA888ABXXXXVCJd+fHG73fzrv/4rjY2NLFy4kJaWFvbs2cO0adOora2ls7MT6LvwXnrppcTFxTFr1izy8vIGrPW1trby3nvvqRfjCRMmcNVVV2E0Ggf1HHw+H0ePHsXtdhMREUF3dzder5fOzk5SU1Nxu93U1dWh0+mIjY2lpaUFk8nE5MmT0Wq1tLa2UldXh9PpRKPRqOV1uVx0dXURERFBamoqGo0Gk8lESUkJdrt9xAOEz+dTvwNFUdBqz73l3m6309raitlsxmq1BryHoihUVFTQ09OD0WgkMTGRiIiIQSv/mWgURVGG5UhDLD09nddee428vDy8Xi8vvvgic+bMYebMmbjdbjZs2EB+fj65ubk4HA42btzI/Pnzyc7OxmazsWnTJi655BKysrLo6enhlVde4bLLLiMzM5POzk5ee+01Fi1aREZGBu3t7WzevJkrr7yStLQ0WlpaeOutt1i8eDGpqak0NTXx9ttvs2TJEpKTk2loaGDr1q1cd911JCYmUltby7vvvsv1119PfHw81dXVvPfee9xwww3ExsZSUVHBjh07+PrXv47VaqWsrIydO3dy0003ERkZyfHjx9m1axfLly8nPDycY8eO8fHHH3PbbbcRGhrK0aNH+eSTT/jggw9ITEzEaDSi1WrZt28fiqIwe/ZsDAYDRUVFLF++nNzcXD799FMMBgO7du1i4sSJZGRk8J3vfAeAzz77jOLiYlauXAnAJ598Qnl5Od/61rcA2L17NzU1Ndx2220A7Nq1i4aGBpYvXw7ABx98QGtrKzfffDMA27dvp6uri69//esA/O1vf8Nut3PjjTcCsG3bNtxuN3/3d38HwNatWwG47rrrUBSF559/nq6uLhYuXIjJZGLv3r3ExcXxta99DYDNmzcTERGhBrmXX36ZsLAwrr/+egBeeeUVIiMjKSgoICoqik2bNpGSksKll14KwEsvvUR6ejoLFiygubmZd955h6ysLAoKCgB44YUXyMnJYe7cuQAUFhaSl5cX9Ld37Ngx3nzzTfR6PUVFRfzd3/0dLS0tNDY28sUXXxAaGsqMGTPUzzU7OxtFUejs7CQ3N5cvv/yS+Ph4pkyZQmRkJJWVlVRWVlJXV0dnZyeTJ08mKiqK4uJiuru7MZlMTJw4kZqaGmw2G2azmdTUVGpqavB4PJjNZhISEqiqqsLhcBAWFkZycjKVlZU4nU7Cw8NJSkqioqICl8uFxWIhLS0Ns9nMsWPHyM/Px+12M3HiRAwGA11dXTQ3NzNp0iSsVisOh4PS0lLmzJmDVqulrKyM9vZ2FixYgE6no6ysjMrKSjIyMggPD6e2thabzcZ9991HS0sL77zzDh0dHeTn5xMWFkZdXZ36funp6bz33nvYbDbuvPNOPv/8cw4dOkRbW5s6SbapqQm73c6kSZPQ6/VotVpqa2uJi4vDaDRSX19Pb28vqampANTX1+P1eklJSQGgrq4ORVGYOnUqPp+P0tJSAJKSknC73ezbt4/Ozk5qa2sBmDRpEnl5eaSmpqLX6ykrK8Pr9RIfH49Op6O8vByj0UhCQgIAx48fp6qqSv0MJk6cSExMDLm5uSiKQlFREVFRUZjNZnw+H+Xl5cTHx6vXqc8//5ykpCRiYmLo6OigpKSE+Ph4MjIyuOSSS87rurdu3brTziGTGsRFyG6343Q6cblcuN1ujh8/js/nC9inpaWFjo4OHnjgAXQ6HUVFRXi9XkpKSli6dCnFxcWsWbOGyMhIDh48iNFo5M4770Sn07Fz504mTpyo3hn19vZSW1tLaWkpkydPBsDhcLB582ZuuOGGQT8/l8tFaWkp//M//8O3vvUtnE4nRqMRRVF46qmnKCsrw+fz8be//Q2AiRMnAlBdXc3ixYvp7e3F7XbzzDPPqJ9DT08PO3bsICIiArfbTUNDA8888wxLliwhOjoagK6uLsLCwtRy7Nu3j0cffZR58+aRmZmJ2+3G6XTi9XqprKwkOzub0NBQALq7u9m+fTsej4f29nYOHjzIyy+/zMGDB5kyZQotLS3ExMTwl7/8hczMTOx2O9/4xjdoa2ujoaGBhoYGbrrpJm699dagNycLFiwIuDm5++67KS8v580338Rut3PppZditVpxOp00Nzczffp0TCYTTqeTlpYW7rzzTnJzc9VmxZycHIxGo3rnmpubS0hICDabjba2NmbMmIHBYKCnp4eqqipKSkqYOnUqe/fuJT4+nt27d+PxeIiMjCQpKQm9Xk9KSgoVFRVUV1ezfft2fD4fVquVuLg4SkpKcDgcuN1uYmNj2blzJwAxMTHExMTw7W9/m97eXqKjo7FarWzbtg2AlJQUzGazepMQHx+PxWLhzjvv5Lvf/S7t7e1UVlayfft2ABISEggNDeX9998H+i7s06ZNU38j/t9CSUkJOp2O8PBwfD4fH3/8MTfddBM9PT14vV5iYmJwu90UFxfjdrt5//33cbvd5OTkkJWVxeWXXw5AW1sbRqORsLAwurq68Pl86HQ6jh8/zhVXXEFFRQWHDh1i+/bt6g1EdnY2K1euVG/SzGYzhw8fZsuWLWRlZWG1WklJScHtdlNZWYnZbGb//v288cYbTJ06FZvNxsSJEzGbzVRUVBAREYHdbmco7vXHTA0iPz9/XMykdrvd3H333bS1tZGcnMzkyZPZuXMn4eHhxMTEcOuttxIfH8/DDz/MggUL+Jd/+Re1+utyuejp6VEviMFUVlZy//33Y7FY8Hg8GI1G9Hq9OhV/xowZrFq1ihdeeIH9+/dTUFDAQw89FNBc8emnn/L000+zbt06YmJizun82tvb+ed//meam5sBiI6Opq2tjVtvvZXo6Gj++Mc/csstt3DzzTdTWVmJ1+vF4/Hw4YcfsnPnTjwej/peISEhpKSkkJycTGpqKp999hlerxeNRsP06dPxer28/fbbTJkyBavVSlFRETNmzOAHP/gBERER/Nu//Rvl5eU4HA7i4+NxOp10dnYSEhKCy+UiNDSUa6+9ls7OTnbu3Km2v/tFR0dz4403snjxYsLCwtDpdGzZsgWLxcJll12GXq9Xv1Ov14vJZDqnz2owORwOFEUZsH/D7XbjdrupqalRm4D8NycREREkJCScU5OP1+tl165dlJWVERkZyeLFi4mMjMTtdrNr1y4qKiqIiopi1qxZfP7550ybNo26ujqqqqrIzc0lKiqKKVOmAH1NMQ0NDdhsNiIjI3G5XFRXV2OxWIiOjsZkMqn7+C97/u8SwGg0qk03BoOBuro6YmJisFgsVFZWEhsbq9Y8/J3rzc3NuFwucnNzgb6AUV1drX6fTqcTg8GAVqvF4XCo5x0eHq7+H9Pr9URFRan/T+12uxqoHA4HERERWK3WgKZEp9NJR0cHPp9PbZY639/Pma6dEiBGiebmZrq6utQ79NPZtWsXv/rVr/j617/Oli1b8Hg8LF68GEVRKCkpobGxkdDQUIxGI7/5zW8IDw8/57J89NFH7Nu3D4PBQENDA1VVVfzTP/0TZWVlvPbaa/h8Prq6usjJyeHIkSOkp6fjdDqZMWMG3/3ud7n//vtpaGjgoYce4rLLLjvjsbZv305tbS3f/va3+eKLL3juueeoq6vjZz/7GRUVFWzdupW4uDj27NkDQHZ2Nr/4xS+CdqK3trZy+PBhFEVBp9Mxc+bMAdtqd+zYwf/+7//i8/nIzs7mww8/xOfzodfr8Xg8rF69moSEBDZv3ozJZCIzM5O2tjZmz57Nrl27+OijjwgJCeHaa6/lmmuuCfhParVa0el0aiAYTdxuN62trYSEhBAVFcXhw4cByM3NHbBPo7KyUq0R6XQ6dDodMTExg96nMZDa2loaGhqYOXMmlZWV2Gw23G43RqMRp9Op7peQkKA2IUFfIPFf9vxNUM3NzWRlZQUEyNLSUjo7O9FqtWrwnzJlCp2dnTQ1Nan7JSUlMWHCBPWxz+ejp6eH8PBwbDYbx48fx+v1kpGRQWRkJBqNZlSNFpQAMcr5fD7+/d//ncbGRr7//e8zefJk9QJYVVVFb28v2dnZAKxdu5ba2lrWr1/PZ599xoEDB7jrrrvQ6/W0trZy//334/P5+OUvf8mkSZMGvazl5eX88z//MxqNhv/+7//m0KFDPPXUUyQmJlJVVUV4eLg6Tv2WW27hjjvuOO17bd26lWeeeQaA66+/nrfffpvIyEi+//3vq+390Pcfes+ePWi1WmbOnDmkF6Ivv/ySkpISmpqaaG9v5/777z/j8To6OjAYDAFNU/7nKysr1drKSAYJRVGw2+14vV7a29txu91qMyUQcEH1XyRPN4rN4XBw+PBh4uPj1bvpkdLd3U1xcTFarVZtzoqMjCQ6OhqPx4OiKGg0mrOq0fhHjJ3M7XZTW1tLV1cX6enpVFVV4XQ6URSFmJgY4uLicDgc/TqVT+VvDrZYLBd8zkNBAsQo5vV6OXjwoFpN93q96HQ6pk2bRlhYGD/5yU/48ssv+dnPfkZzczNPP/00K1as4Fvf+haKoqhtnn7+gGK32zEajaSlpfW7eF2offv2YbPZ1NqBf6jhjh07OHDgABkZGbz33ntERUXx2GOPYbPZ2LZtG83NzVxxxRWEh4dTWlrKr3/9a+bMmaNmk5w6dSq/+MUvCAkJGdTyDgVFUWhqaiIqKqpfAHG73XzxxReYTCYcDgehoaGEhoai0WiIjIzEYrGod5A+n4+mpiasVuuQBL6enh5KS0vVpjeNRoNer0dRFDIyMtT+JaPRiMvlwmAw4HK5iIiIICQkRL3b9tfKfD4f7e3tah/FSGtra6OyspKUlBTi4uKG9Fhut1vttJ46dep5jVYajSRAjGLHjh2ju7ubv/3tbyxcuJDPP/+cSy65hNjYWFJSUlixYkVAdXnGjBn867/+K9CXptftdjNjxgz1x9rR0UFpaSkRERE4HA48Hg+TJ09W//NHRkYOy3n9/ve/55NPPmHZsmW89tpr2Gw2tdnGb/LkyTz++ON89tln/PnPf+ZnP/tZQFPASFMUhba2NqKiovrdUXd1dVFSUoLFYmHKlCkBTQY1NTU0NjaSk5NDd3e3OjLG5/OhKApRUVFqn4bNZqO5uRm9Xs/UqVPPa46D2+2mra2Nzs5OJk6cqDZzOZ1Ojh49il6vZ8KECRgMBsxms1qb0Wg0KIpCS0sLYWFhNDQ00N7eTmRkJHa7XR2koNVq0Wg06t2z1WolIyPjAj7ZweWvKQzXsYBR1UR0oc507Rx9jaPjSHNzMz09PXz44YdkZmYyd+5cfvOb3zBt2jR0Oh3d3d1Mnz6dWbNmAWAymbjmmmswGAwcPnwYt9uNz+ejpaWFrq4uYmNj1VEPkydPxuPxcPz4cUpKSoC+MfRTp07FbrcTHR09pD/yyZMn8+6777Jhwwbmz5/PbbfdRkJCAgcOHMDr9aLVapk7dy4mk4nLL7+cyy67bNT9p2tqaqKmpgar1Up6ejoAjY2NtLa2qgG5u7ubL7/8EkVRiI6Oxuv10tTUpHaKmkwm9c7WX1uora2lo6NDPY7ValXv9LOzs/sFI5vNhkaj6Rc8fD4fVVVVtLa2qs9VVVWRmJiIVquluroagMzMzNN2YGo0GrV8qampREdHq+3kp+rp6aGmpobExMRz+RiH3HD+bkbbb3SoSQ1iBP31r39l0qRJhIaGqn0MP/3pTzGbzSxdulTdz2KxMHXqVNrb26mpqcFsNtPZ2UlGRgbV1dW43e6A9z15+KXH46G6ulodA+6XlZV1Xh3YZ+vYsWM8+OCDzJo1i8cee+yiqI7bbDacTqfaBu/vuPUPe/R6vXR0dKh33omJiepIEp1Op6aMiIiIYNKkSadtKuvs7MTtdmMymejp6SE+Ph6bzcaxY8cICwsjKSlJbYZqa2ujvLwcjUZDSkqK2qzj8/lobm6mt7eX+Ph4YmJi6O3tpaqqKuBY/nkSQpyO1CBGEUVRKC4upqSkhJaWFlJSUpg2bZq6fe7cufz5z39m2rRpVFRUkJiYSFZWljpiQ6/X09nZiclkIioqCpvNRkNDAzExMXR3dxMdHa0GBwC9Xq/e/fovcG63m46OjiENEFOmTOHee+/lsssuGzXBwT+65OQ+AD+v16vWBOCr5pfs7GxaW1tpbm5Go9EwYcIEYmNjaW5uJiEhgaSkJPU9PB6P2sZ/Jic38/m/g/DwcCZNmkRtbS0lJSWEhoYSHh5OU1OTOlbfXyPwMxgMpKWlqUOJzWazOorH4/Gg1WolOIgLIjWIYeZvnwbUsdIzZ85Ut7e1tfHb3/6W4uJient7ufTSS1m6dCk+nw+LxUJmZibNzc2Eh4cTFhaG2+2mubmZxMTEAYfP+b/q48eP43A4zmpI41jQ3NxMS0sLLpcLj8dDSkqKOrPVz9+nMHHiRHWsf1xcnBpsT82TNFR8Ph+tra00NjbidDqJiooiPT0djUajzlUA1HQT4+H7E0NrxGoQW7du5Z/+6Z/wer3cddddPPzwwwHbH3jgAXUGpM1mo6mpSW2b1el0agqCiRMn8sYbbwxlUYeNP39MQkICOp0Oq9UasD06OprHHnsMu93Ou+++y/Tp04mIiKCpqYm0tDS0Wm3Axc1gMATcxZ6J/2JitVrVseyxsbGj4iLjTy9hNBoH7cJXW1tLT08PPT09hIaGYrFYsNlstLa2qp9hT08Pra2t6vGio6ODBoHhSl6o1WqJi4sjNjYWu92O2WxWyzaak/SJsWnIAoTX62XNmjW8++67pKSkUFBQwLJly5g+fbq6z29+8xv179///vfs379ffWw2mzlw4MBQFW/I2Ww2Ojs7AybQ1NXV4Xa71bQENTU1px2CajabWbZsmfo4Li5u0C7kVquVlpYWtb36QocHDsYokq6uLnUI4cSJEy+4TD6fj4aGBkJCQkhISCA5ORmNRkNTUxPV1dV0d3fjcDiorq5W78r9s51HA41GE9BUKMRIGLLG4T179pCZmUlGRgYhISGsWLGCzZs3n3b/l156SU3+Nhb813/9F/fddx8vv/wymzZtwm63s2HDBiIjI8nIyCAuLo6UlJSzbiMezLt8nU5HVlaWmjb5QrS1tXHgwIELfp/Ozk61/d4/0c7lctHc3HxeOWb8HcYpKSmkpKQE1J40Gg3FxcVUVVURFhamzrYerROZhBgpQ1aDqK2tDZhpmZKSwqeffhp038rKSsrLy7n66qvV5xwOB/n5+ej1eh5++GG+8Y1v9Hvd+vXrWb9+PYCau2c0cDqdfPLJJ+h0Ol588UWgb7bqoUOHuPrqq4mIiOjXVDTc/JO2mpqacLvdKIpyzhPUOjo6KC8vB/qGf57aXBaMy+WisbGRxMREdUSOv3kpIiJCnfULfcNMGxsbMZlM53zxttlsAP3uwg0GA5mZmTidTkwmE+Hh4bhcLo4fP35W5RdiPBkVo5g2btzI8uXLA6r3lZWVJCcnU1ZWxtVXX82MGTP65SlavXo1q1evBvo6WkaLoqIinE4njz76KFarlX//93/nlVdeUS9yw52z5nQiIyNpbGxU8xdNmTLljCObvF4vVVVVmM1moqKiqKioIDQ0lKioKOrq6tQ289NxuVwcO3YMl8ulDtuEr1IRJCYm4nA46O7uRlEUtSbR0tKCxWJR50+cTW3KZrOh0+mCBr1T8zMZjUZycnIGfE8hxpsha2JKTk4OGJZXU1NDcnJy0H03btzYr3nJv29GRgZXXnllQP/EaLdr1y4iIyOZNWsW6enp5Ofn09nZqWZRHS2pJMLCwtREZHq9npKSEnVOhdPpVNNm2+12Ghsb+fLLL2lra6O2tlYNKunp6WpHd3FxccBcC5fLhaIoeL1evF6vmvkyNDSU1tZWFEXBZrNRVlYG9F24zWYziqLgcDjUCWLt7e04HA6OHDnC0aNH1eybTqeTkpKSoGsUn9rBK4Q4d0NWgygoKKCkpITy8nKSk5PZuHEj//u//9tvvy+//JL29nYuueQS9bn29nY1I2lLSwsfffQRDz300FAVdVD5FxW5/PLL1RpRfn4+b7/9trp+7mgJEFqtlokTJ6LT6TCZTBw+fJiGhgbCw8MpLy/v1/bvz2Zqs9lwuVxMmDBBPZfMzEwaGxupq6vD5/OpAcFkMuFyudQFUMLDw0lISKC0tJQjR47gcDjQ6XRMnjxZHb8PfbUGRVFISkqivr6eI0eOqKt1+RfRaWxsVPefMmWKms4C+moQQ52bR4ixbsgChF6vZ926dSxduhSv18uqVavIyclh7dq15OfnqyN0Nm7cyIoVKwLu9I4ePco999yjZml8+OGHA0Y/jWaHDx/Gbrczb948oC8Vg0ajYdWqVSQkJKDX60fNSBkgYL2GmJgYmpqaaGpqIiwsjPj4eDweDzqdDovFogaDYPmcIiIisFgslJaW0tDQAPQNGfUnDfSvHZCSkkJkZKTaN5CcnExsbKw6ucyfEsKfPiIuLo6wsDDKysrUpRbLysqora3FbDYTERFBW1sbTU1NNDc343A41Eluw7UsoxBjlUyUG2TPPvss77zzDhs2bMBoNFJWVqauJ+xfZ8CfW2m0cTqdVFZWEhERQXx8/HnNgPY3D2m1WrWvxefzcejQIdxuN7m5uQP2wXzxxRe4XC4iIyPJzMxU3/fkDKj+TmaXy8WhQ4cA1L4Rr9dLdHT0oGexFWIsklQbw+izzz5j5syZ6HQ6Ojo6aG9vJyEhgZiYGI4cOTJs2VTPh9FoVJvBzlewpHJarZbU1FS6urrOqoM+PT0dr9cbUAM4uYap1WrVYxiNRlJSUtBqtaNm0p8QY4UEiEHkcDhoaGhgyZIl1NXVqUNvY2NjMZlMzJ49e9xewKxW61kPIz3XHFEjOVxYiLFMAsQg8Der+Ndtjo2NVZtAJk+erLarj5akdUIIcTYkQAyC9vZ2dcJYcnIyMTExalbNkVyIXgghLoTc0g4C/wgd6Bul4w8Qo2FJRiGEOF8SIAaB3W4nNDRUXVjGarXi8XhGzXwHIYQ4HxIgBoF/1q7dbicuLk6d5yA1CCHExUwCxAVyu914PB7MZjNdXV1q8xJIgBBCXNwkQFwgh8MB9E3SamlpITw8HKfTCYyelBpCCHE+JEBcIH9qarPZTH19PRqNRk0eJzUIIcTFTALEBfInmwPU7LX+/EujKeeSEEKcKwkQF8jlchESEkJHR4eaYM7pdGIwGMbtrGkhxNggAeIC+ec7+NcsGCO5D4UQQgLE2dq7dy/r1q3r97w/QPgztlosFpKTk9XV0oQQ4mIlAeIs7dixg23btgWs4awoihogOjo6AIiKiiIxMVHWNxZCXPSGNEBs3bqVrKwsMjMzeeKJJ/ptf+CBB8jLyyMvL4+pU6cSFRWlbnv++eeZMmUKU6ZM4fnnnx/KYp6VqqoqAI4cOcLBgwfp7OxU5zuEhISoNYjRnM5bCCHOxZAl6/N6vaxZs4Z3332XlJQUCgoKWLZsWcDKcL/5zW/Uv3//+9+r6063tbXx2GOPUVRUhEajYe7cuSxbtmzE7sq9Xi81NTVA39ra2dnZ9PT0qMNY/U1MJpNJkvMJIcaMIatB7Nmzh8zMTDIyMggJCWHFihVs3rz5tPu/9NJLfOtb3wLgnXfeYcmSJURHR2O1WlmyZAlbt24dqqIOqL6+HrfbjU6no62tDehb8/jkGdMdHR1SexBCjClDFiBqa2tJTU1VH6ekpFBbWxt038rKSsrLy7n66qvP+bXDwd+8NH/+fHXoand3Ny6XC/iqBiEBQggxloyKTuqNGzeyfPnyc55Ytn79evLz88nPz1dXb7tQiqLg9XoDnvMHiL//+79X10iGvloEfBUgTu5DEUKIi92QBYjk5GR1ZjH0td0nJycH3Xfjxo1q89K5vHb16tUUFRVRVFREXFzcoJT7//7v//jud7+Lx+NRn6uqqiIhIYHk5GRSU1PV9R86OzvVCXFSgxBCjDVDFiAKCgooKSmhvLwcl8vFxo0bWbZsWb/9vvzyS9rb27nkkkvU55YuXcq2bdtob2+nvb2dbdu2sXTp0qEqaoDDhw/T1tZGZWWl+lxVVRWTJk1Sh7UWFxejKAoejweDwYDP55MAIYQYc4YsQOj1etatW8fSpUvJzs7mtttuIycnh7Vr1/LGG2+o+23cuJEVK1YEpKWIjo7mpz/9KQUFBRQUFLB27Vqio6OHqqgBKioqACgpKQH6JsL5+0S8Xi8+n4+Wlhbq6+uxWq3Ex8fT29uL1+uVACGEGFOGdE3q66+/nuuvvz7guZ/97GcBjx999NGgr121ahWrVq0aqqIF5Xa7qaurA+D48eNA3wgmr9fLxIkTaW9vB/r6HmpqarjxxhsB1CGw0gchhBhLBqxBvPnmm/h8vuEoy4irra3F6/Wi0+nUGoS/gzo+Pl79u7e3V10HAlBnUUsNQggxlgwYIP7yl78wZcoUHnroIb788svhKNOI8fc75OfnU1lZidPppKqqCq1Wi8FgwGg0kpWVhcvlUteBAGQWtRBiTBowQPzP//wP+/fvZ/Lkydx5551ccsklrF+/nu7u7uEo37CqqqpCp9Nx9dVX4/P5OHLkCJWVlUyYMAGXy4XZbCY8PFxdf9qvq6sLgIiIiJEquhBCDLqz6qSOiIhg+fLlrFixgvr6el577TXmzJnD73//+6Eu37Dq7e1l/vz5zJ07F7PZzIcffkhFRQUTJ07E5XJhNBoBMJlMAU1M/mGv4eHhI1JuIYQYCgN2Ur/xxhv8+c9/5vjx43znO99hz549xMfHY7PZmD59Ovfff/9wlHNYFBQUAH0T3xYsWMD777+Pz+dj+fLlKIqi5lkym81qyg1AzcvkDyBCCDEWDBggXn31VR544AEWLVoU8HxoaCh/+tOfhqxgI8ntdnP55Zezfft2srOzKSgooLS09LQ1iJ6eHsLCwkaquEIIMSQGDBCPPvooEyZMUB/b7XYaGxtJS0tj8eLFQ1q44eZfPrS3t5fZs2ezYsUKFi9erOZcOrkGcXIfRG9vrzQvCSHGnAH7IG699Va02q920+l03HrrrUNaqJHi72zu6elBp9Nx++23k5CQgNPpRKvVotf3xdNTA0RPT48ECCHEmDNggPB4PISEhKiPQ0JC1Dvqsca/nrS/09lms6EoCg6HA6PRqM72NpvNeDweNd13b2+vNDEJIcacAQNEXFxcQGqMzZs3ExsbO6SFGmk2m422tjaOHj1KS0sLNpuN0NBQdbu/qcnfDyE1CCHEWDRgH8Qf/vAHVq5cyfe+9z0URSE1NZUXXnhhOMo27DQajdqfUF5eDkBraysej+e0AcJisUgNQggxJg0YICZPnswnn3xCT08PMLbH+vsDxOTJk2loaECv16vNTScHAH+wsNvt+Hw+6aQWQoxJZ5Wsb8uWLRw+fDhgaOfatWuHrFAjRaPRoCgKSUlJREZGYrfbqaqqQqPRYDab1f38NQi73Y7D4cDn80kNQggx5gwYIO69915sNhvbt2/nrrvu4pVXXmHevHnDUbZh5++E1mg0hIeHqyvcmc3mgJFc/mBht9vVmpUECCHEWDNgJ/Xu3bt54YUXsFqtPPLII3z88ccUFxcPR9mG3clrUkBfTSEkJASLxdLveejrg5A0G0KIsWrAAOG/GIaGhlJXV4fBYKC+vv6s3nzr1q1kZWWRmZnJE088EXSfTZs2MX36dHJycrj99tvV53U6HXl5eeTl5QVdiW4onBogNBoN2dnZJCUlBTwfrAYhAUIIMdYM2MR044030tHRwYMPPsicOXPQaDTcfffdA76x1+tlzZo1vPvuu6SkpFBQUMCyZcuYPn26uk9JSQmPP/44H330EVarlaamJnWb2WzmwIED53dW5+nUAAGok+NOJk1MQojx4IwBwufzsXjxYqKiorjlllu44YYbcDgcZ7XuwZ49e8jMzCQjIwOAFStWsHnz5oAA8eyzz7JmzRqsVivQtyjPSAoWIIKRJiYhxHhwxiYmrVbLmjVr1MdGo/GsF8Xxr+Psl5KSQm1tbcA+xcXFFBcXs3DhQhYsWMDWrVvVbQ6Hg/z8fBYsWMDrr79+Vse8UFqt9qyChH9WdWdnpzQxCSHGrAGbmBYvXsyrr77KzTfffNZ32GfL4/FQUlLCjh07qKmpYdGiRXzxxRdERUVRWVlJcnIyZWVlXH311cyYMYPJkycHvH79+vWsX78egObm5gsuz9men1arJSEhgddeew2LxdJvGKwQQowFA3ZS//GPf+TWW2/FaDQSERGBxWI5q5XTkpOTqa6uVh/X1NSQnJwcsE9KSgrLli3DYDCQnp7O1KlT1bWg/ftmZGRw5ZVXsn///n7HWL16NUVFRRQVFREXFzdgmQZyLgHwqaee4o477sBgMJCUlBQwDFYIIcaCAa9q3d3d+Hw+XC4XXV1ddHd3q1lPz6SgoICSkhLKy8txuVxs3Lix32ikb3zjG+zYsQOAlpYWiouLycjIoL29HafTqT7/0UcfBfRdDJVzuciHh4dzyy238Nxzz/H0008PXaGEEGKEDNjEtHPnzqDPn7qAUL831utZt24dS5cuxev1smrVKnJycli7di35+fksW7aMpUuXsm3bNqZPn45Op+PJJ58kJiaG3bt3c88996DVavH5fDz88MPDFiDOtRlNr9cHHekkhBAXO43iz3F9GjfeeKP6t8PhYM+ePcydO5f3339/yAt3LvLz8ykqKrqg9/j0009pb2/nuuuuG6RSCSHE6Hama+eAt75vvvlmwOPq6mp+8IMfDErBRpvzqUEIIcRYdc49qykpKRw9enQoyjKivF6vBAghhDjJgDWI+++/X71o+nw+Dhw4wJw5c4a8YMPN5/ONdBGEEGJUGTBA5Ofnf7WzXs+3vvUtFi5cOKSFGgkejwc4t6GuQggxlg0YIJYvX47JZFJTX3u93n5LcI4FXq8XkAAhhBB+A/ZBLF68GLvdrj622+1cc801Q1qokSABQgghAg0YIBwOR0CeofDwcGw225AWaiT4+yAkQAghRJ8BA0RYWBj79u1TH+/du3dM5h2SGoQQQgQasA/i6aef5tZbbyUpKQlFUWhoaOAvf/nLcJRtWPkDhORUEkKIPgMGiIKCAr788kuOHTsGQFZWFgaDYcgLNtz8AUIIIUSfAW+X//M//5Pe3l5yc3PJzc2lp6eHZ555ZjjKNqz8fRBSgxBCiD4DXg2fffZZoqKi1MdWq5Vnn312KMs0IqQPQgghAg0YILxeLyfn8/N6vbhcriEt1EiQPgghhAg0YB/Eddddxze/+U3uueceoG8Boa997WtDXrDh4nA4ePvtt4mOjsZisUiAEEKIEwa8Gv7yl7/k6quv5g9/+AN/+MMfmDFjRsDEuYudy+WisLCQ0tJSQJqYhBDCb8AAodVqmT9/PmlpaezZs4f333+f7Ozs4SjbsPAv9uPPxSQ1CCGE6HPaq2FxcTGPPfYY06ZN4/7772fixIkAbN++ne9973tn9eZbt24lKyuLzMxMnnjiiaD7bNq0ienTp5OTk8Ptt9+uPv/8888zZcoUpkyZwvPPP38u53RO/AHC7XYDEiCEEMLvtH0Q06ZN4/LLL+ett94iMzMTgN/85jdn/cZer5c1a9bw7rvvkpKSQkFBAcuWLQtYOrSkpITHH3+cjz76CKvVSlNTEwBtbW089thjFBUVodFomDt3LsuWLcNqtZ7veZ7WqTUIaWISQog+p71d/utf/8qECRO46qqruPvuu3nvvfcYYHXSAHv27CEzM5OMjAxCQkJYsWIFmzdvDtjn2WefZc2aNeqFPz4+HoB33nmHJUuWEB0djdVqZcmSJWzduvV8zm9AWq0WnU4nTUxCCHGK014Nv/GNb7Bx40a+/PJLrrrqKp5++mmampr4x3/8R7Zt2zbgG9fW1pKamqo+TklJoba2NmCf4uJiiouLWbhwIQsWLFCDwNm8FmD9+vXk5+eTn59Pc3PzwGd7Gnq9XgKEEEKc4qyS9d1+++28+eab1NTUMHv2bH75y18OysE9Hg8lJSXs2LGDl156ibvvvpuOjo6zfv3q1aspKiqiqKiIuLi48y6HXq+XeRBCCHGKc7oaWq1WVq9ezXvvvTfgvsnJyVRXV6uPa2pqSE5ODtgnJSWFZcuWYTAYSE9PZ+rUqZSUlJzVaweT1CCEEKK/IbsaFhQUUFJSQnl5OS6Xi40bN7Js2bKAfb7xjW+wY8cOAFpaWiguLiYjI4OlS5eybds22tvbaW9vZ9u2bSxdunSoiorBYJAahBBCnGLAmdTn/cZ6PevWrWPp0qV4vV5WrVpFTk4Oa9euJT8/n2XLlqmBYPr06eh0Op588kliYmIA+OlPf0pBQQEAa9euJTo6eqiKGtDE5F9aVQghxjuNci5Dk0ax/Px8ioqKzuu1a9aswWKxcOuttxIVFcXkyZMHuXRCCDE6nenaKe0p9NUg/PMfpAYhhBB9JEDQFyD8fQ/SByGEEH3kaogECCGECEauhgQGCGliEkKIPhIgkBqEEEIEI1dD+uZBSCe1EEIEkgBBYA3Cn91VCCHGOwkQ9NUgpIlJCCECydUQ6aQWQohgJEAgAUIIIYKRAEHgTGrpgxBCiD4SIJBhrkIIEYxcDfkqQPh8PgkQQghxglwN+WoU0xhJbCuEEINCAgSBNQghhBB9hjRAbN26laysLDIzM3niiSf6bS8sLCQuLo68vDzy8vJ47rnn1G06nU59/tSV6AabvwYhAUIIIb4yZEN2vF4va9as4d133yUlJYWCggKWLVvG9OnTA/b75je/ybp16/q93mw2c+DAgaEqXgD/KCZpYhJCiK8MWQ1iz549ZGZmkpGRQUhICCtWrGDz5s1DdbgLIk1MQgjR35AFiNraWlJTU9XHKSkp1NbW9tvv1VdfZebMmSxfvpzq6mr1eYfDQX5+PgsWLOD1118Peoz169eTn59Pfn4+zc3N511Wf4CQGoQQQnxlRDupb7zxRioqKjh48CBLlizhjjvuULdVVlZSVFTE//7v//KDH/yA0tLSfq9fvXo1RUVFFBUVERcXd97lkAAhhBD9DVmASE5ODqgR1NTUkJycHLBPTEwMRqMRgLvuuou9e/cGvB4gIyODK6+8kv379w9VUdV039LEJIQQXxmyAFFQUEBJSQnl5eW4XC42btzYbzRSfX29+vcbb7xBdnY2AO3t7TidTgBaWlr46KOP+nVuDyapQQghRH9DNopJr9ezbt06li5ditfrZdWqVeTk5LB27Vry8/NZtmwZv/vd73jjjTfQ6/VER0dTWFgIwNGjR7nnnnvUjuOHH354SAOETJQTQoj+NMoYuSrm5+dTVFR0Xq/9/PPPOXjwIBMmTOCaa64Z5JIJIcTodaZrp8ykRuZBCCFEMBIgkD4IIYQIRgIEgem+hRBC9JGrItJJLYQQwUiAQJqYhBAiGAkQfFWDEEII8RW5KiKjmIQQIhgJEEgntRBCBCNXRSRACCFEMHJVRDqphRAiGAkQSA1CCCGCkasioNFoJEAIIcQp5Kp4gk6nG+kiCCHEqCIB4gSpQQghRKAxc1VsbW3lwIEDAHi9XgoLCzl48CAAbrebwsJCDh06BPStd11YWMjRo0fxer0cOnSIQ4cO0dHRAUBPTw+FhYUcP34cgM7OTgoLCykrKwP6FjQqLCykoqIC6FvUqLCwUF1Br6mpicLCQnUN7oaGBgoLC2loaAD61usuLCykqakJgOrqagoLC2lpaQGgoqKCwsJC2tvbASgrK6OwsJDOzk4Ajh8/TmFhIT09PQAcO3aMwsJCbDYb0LeeRmFhIQ6HA4BDhw5RWFiI2+0G4ODBgxQWFuL1egE4cOCAuhYHwN69e3nhhRfUx5999hkbNmxQH3/yySe89NJL6uPdu3ezadMm9fGuXbt45ZVX1McffPABf/3rX9XH27dvZ/Pmzerjv/3tb7z55pvq423btrFlyxb18datW9m6dav6eMuWLWzbtk19/Oabb/K3v/1Nfbx582a2b9+uPv7rX//KBx98oD5+5ZVX2LVrl/p406ZN7N69W3380ksv8cknn6iPN2zYwGeffaY+fuGFFwJWPywsLDyv3x6AzWajsLCQY8eOAfLbk9/e8P/2zmRIA8TWrVvJysoiMzOTJ554ot/2wsJC4uLiyMvLIy8vj+eee07d9vzzzzNlyhSmTJnC888/P2RlVBQFt9uNoihYLJYhO44QQlxshmzBIK/Xy9SpU3n33XdJSUmhoKCAl156KWBluMLCQoqKili3bl3Aa9va2tRFLDQaDXPnzmXv3r1YrdbTHu9CFgwSQojxakQWDNqzZw+ZmZlkZGQQEhLCihUrAqp2Z/LOO++wZMkSoqOjsVqtLFmyJKCaJ4QQYugNWYCora0lNTVVfZySkqK2i57s1VdfZebMmSxfvlxtRz3b1wohhBg6I9pJfeONN1JRUcHBgwdZsmQJd9xxxzm9fv369eTn55Ofn09zc/MQlVIIIcanIQsQycnJao0AoKamhuTk5IB9YmJiMBqNANx1111q7/zZvBZg9erVFBUVUVRURFxc3FCchhBCjFtDFiAKCgooKSmhvLwcl8vFxo0bWbZsWcA+9fX16t9vvPEG2dnZACxdupRt27bR3t5Oe3s727ZtY+nSpUNVVCGEEEHoh+yN9XrWrVvH0qVL8Xq9rFq1ipycHNauXUt+fj7Lli3jd7/7HW+88QZ6vZ7o6Gh1TG50dDQ//elPKSgoAGDt2rVER0ef8XgVFRXk5+efd3mbm5vHTS1kPJ0ryPmOZePpXGFoztc/pyaYIRvmerEZT8Nkx9O5gpzvWDaezhWG/3zHzExqIYQQg0sChBBCiKAkQJywevXqkS7CsBlP5wpyvmPZeDpXGP7zlT4IIYQQQUkNQgghRFDjPkAMlHF2LEhLS2PGjBnk5eWpQ4Hb2tpYsmQJU6ZMYcmSJWp654vRqlWriI+PJzc3V33udOenKArf//73yczMZObMmezbt2+kin1egp3ro48+SnJyspoV+e2331a3Pf7442RmZpKVlcU777wzEkW+INXV1Vx11VVMnz6dnJwcfvvb3wJj8/s93bmO6PerjGMej0fJyMhQSktLFafTqcycOVM5fPjwSBdr0E2aNElpbm4OeO7BBx9UHn/8cUVRFOXxxx9XHnrooZEo2qD44IMPlL179yo5OTnqc6c7vy1btijXXXed4vP5lI8//liZN2/eiJT5fAU710ceeUR58skn++17+PBhZebMmYrD4VDKysqUjIwMxePxDGdxL1hdXZ2yd+9eRVEUpaurS5kyZYpy+PDhMfn9nu5cR/L7Hdc1iAvJOHux27x5s5r76o477uD1118f2QJdgEWLFvWbSHm689u8eTPf+c530Gg0LFiwgI6OjoAZ/aNdsHM9nc2bN7NixQqMRiPp6elkZmayZ8+eIS7h4JowYQJz5swBwGKxkJ2dTW1t7Zj8fk93rqczHN/vuA4Q4yVrrEaj4dprr2Xu3LmsX78egMbGRiZMmABAYmIijY2NI1nEQXe68xur3/m6deuYOXMmq1atUptbxtq5VlRUsH//fubPnz/mv9+TzxVG7vsd1wFivNi1axf79u3j//7v//jP//xPdu7cGbBdo9Gg0WhGqHRDb6yf3z/+4z9SWlrKgQMHmDBhAj/60Y9GukiDrqenh1tuuYWnn36aiIiIgG1j7fs99VxH8vsd1wHibLPGXuz85xQfH89NN93Enj17SEhIUKve9fX1xMfHj2QRB93pzm8sfucJCQnodDq0Wi1333232swwVs7V7XZzyy23sHLlSm6++WZg7H6/pzvXkfp+x3WAOJuMsxe73t5euru71b+3bdtGbm4uy5YtU9f6fv755/n6178+ksUcdKc7v2XLlvHCCy+gKAqffPIJkZGRalPFxerkNvbXXntNHeG0bNkyNm7ciNPppLy8nJKSEubNmzdSxTwviqLw3e9+l+zsbH74wx+qz4/F7/d05zqi3++gdnlfhLZs2aJMmTJFycjIUH7+85+PdHEGXWlpqTJz5kxl5syZyvTp09VzbGlpUa6++molMzNTWbx4sdLa2jrCJT1/K1asUBITExW9Xq8kJycrzz333GnPz+fzKffdd5+SkZGh5ObmKp999tkIl/7cBDvXb3/720pubq4yY8YM5cYbb1Tq6urU/X/+858rGRkZytSpU5W33357BEt+fj788EMFUGbMmKHMmjVLmTVrlrJly5Yx+f2e7lxH8vuVmdRCCCGCGtdNTEIIIU5PAoQQQoigJEAIIYQISgKEEEKIoCRACCGECEoChBgzWltb1YyXiYmJARkwXS7XsJSho6ODZ555Rn1cV1fH8uXLh+XYA9mxYwc33HDDSBdDXEQkQIgxIyYmhgMHDnDgwAHuvfdeHnjgAfVxSEgIHo9nyMtwaoBISkrilVdeGfTjeL3eQX9PIU4lAUKMaXfeeSf33nsv8+fP56GHHuLRRx/lP/7jP9Ttubm5VFRUUFFRQXZ2NnfffTc5OTlce+212O12AI4fP84111zDrFmzmDNnDqWlpfT09LB48WLmzJnDjBkz1CzADz/8MKWlpeTl5fHggw9SUVGhznx1OBz8wz/8AzNmzGD27Nls374dgMLCQm6++Wauu+46pkyZwkMPPRT0XNLS0viXf/kX5syZw8svv8yzzz5LQUEBs2bN4pZbbsFms6nn/P3vf59LL72UjIyMoAHqs88+Y/bs2ZSWlg7ehy3GHAkQYsyrqalh9+7dPPXUU2fcr6SkhDVr1nD48GGioqJ49dVXAVi5ciVr1qzh888/Z/fu3UyYMAGTycRrr73Gvn372L59Oz/60Y9QFIUnnniCyZMnc+DAAZ588smA9//P//xPNBoNX3zxBS+99BJ33HEHDocDgAMHDvCXv/yFL774gr/85S8BOXZOFhMTw759+1ixYgU333wzn332GZ9//jnZ2dn86U9/Uverr69n165dvPXWWzz88MMB77F7927uvfdeNm/ezOTJk8/58xTjh36kCyDEULv11lvR6XQD7peenk5eXh4Ac+fOpaKigu7ubmpra7npppsAMJlMQF9StZ/85Cfs3LkTrVZLbW3tgCnTd+3axf333w/AtGnTmDRpEsXFxQAsXryYyMhIAKZPn05lZWVAKme/b37zm+rfhw4d4t/+7d/o6Oigp6eHpUuXqtu+8Y1voNVqmT59ekC5jh49yurVq9m2bRtJSUkDfiZifJMAIca8sLAw9W+9Xo/P51Mf++/gAYxGo/q3TqdTm5iC2bBhA83NzezduxeDwUBaWlrAe52rU499uv6Sk8/lzjvv5PXXX2fWrFkUFhayY8eOoO93cjadCRMm4HA42L9/vwQIMSBpYhLjSlpamrpO8b59+ygvLz/j/haLhZSUFHXFMqfTic1mo7Ozk/j4eAwGA9u3b6eyslLd358991SXX345GzZsAKC4uJiqqiqysrLO+1y6u7uZMGECbrdbfd+BREVFsWXLFn784x8HBBQhgpEAIcaVW265hba2NnJycli3bh1Tp04d8DUvvvgiv/vd75g5cyaXXnopDQ0NrFy5kqKiImbMmMELL7zAtGnTgL4+goULF5Kbm8uDDz4Y8D733XcfPp+PGTNm8M1vfpPCwsKAO/1z9f/+3/9j/vz5LFy4UD3+2UhISOCtt95izZo1fPrpp+d9fDH2STZXIYQQQUkNQgghRFASIIQQQgQlAUIIIURQEiCEEEIEJQFCCCFEUBIghBBCBCUBQgghRFASIIQQQgT1/wG68dCZT2YbhgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss and accuracy over training epochs\n",
    "fig = plt.figure(figsize=(6, 6), facecolor='w')\n",
    "axes = fig.subplots(2, 1, sharex=True)\n",
    "axes = axes[:, None]\n",
    "epochs = np.arange(N_EPOCHS)\n",
    "train_loss, valid_loss, train_acc, valid_acc = loss_acc.T\n",
    "\n",
    "ax0, ax1 = axes[:, 0]\n",
    "\n",
    "for j in range(2):\n",
    "    # ls = ['-', '--'][j]\n",
    "    ls = '-'\n",
    "    lbl = ['Truncate dw', 'Truncate w'][j]\n",
    "    c = ['0.3', '0.8'][j]\n",
    "\n",
    "    trunc_loss, trunc_acc = trunc_loss_acc[j].T\n",
    "    ax0.plot(trunc_ranks, trunc_loss, ls, c=c, label=lbl)\n",
    "    ax1.plot(trunc_ranks, trunc_acc, ls, c=c, label=lbl)\n",
    "\n",
    "# Initial and final loss and accuracy\n",
    "c = '0.5'\n",
    "ax0.axhline(valid_loss[0], ls='--', c=c, label='Initial')\n",
    "ax1.axhline(valid_acc[0], ls=':', c=c, label='Initial')\n",
    "ax0.axhline(valid_loss[-1], ls='--', c=c, label='Final')\n",
    "ax1.axhline(valid_acc[-1], ls=':', c=c, label='Final')\n",
    "\n",
    "ax0.legend()\n",
    "# ax0.axhline(0, c='0.5', zorder=-1)\n",
    "# ax1.axhline(0, c='0.5', zorder=-1)\n",
    "ax0.set_ylabel('Loss')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel(\"Truncation rank\")\n",
    "\n",
    "plt.show(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXzD-MitVstx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRmNEmztakGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFeWrLUrakJk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaCYSvajakMx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjcH1W1HakPj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0TyyDQXakeQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZGCWFjCakhG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11915,
     "status": "ok",
     "timestamp": 1597315287705,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "wOhQOEhgVsw6",
    "outputId": "5a8811da-42f1-4bd3-ed13-50430c6a31ac"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/sst_lstm_nlayers_1_nhid_1024_emb_pretrained_fix_dim_100_dropout_00_weight_decay_0000_seed_4375.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d1ffdb78685e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstate_dict_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict_init'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/sst_lstm_nlayers_1_nhid_1024_emb_pretrained_fix_dim_100_dropout_00_weight_decay_0000_seed_4375.pt'"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "SAVE = 'saved_models/sst_lstm_nlayers_1_nhid_1024_emb_pretrained_fix_dim_100_dropout_00_weight_decay_0000_seed_4375.pt'\n",
    "\n",
    "data_file = SAVE\n",
    "with open(data_file, 'rb') as f:\n",
    "    res = torch.load(f, map_location=torch.device(device))\n",
    "    state_dict_init = res['state_dict_init']\n",
    "    state_dict_final = res['state_dict_final']\n",
    "    loss_acc = res['loss_acc']\n",
    "    try:\n",
    "        trunc_loss_acc = res['trunc_loss_acc']\n",
    "        trunc_ranks = res['trunc_ranks']\n",
    "    except:\n",
    "        print('not defined')\n",
    "\n",
    "train_loss, valid_loss, train_acc, valid_acc = loss_acc.T\n",
    "N_EPOCHS = len(train_loss)\n",
    "\n",
    "# Obtain dimensions\n",
    "INPUT_DIM, EMB_DIM = state_dict_init[\"encoder.weight\"].shape\n",
    "OUTPUT_DIM, HIDDEN_DIM = state_dict_init[\"decoder.weight\"].shape\n",
    "N_LAYERS = max([int(key[-1])  \n",
    "                for key in state_dict_init.keys() \n",
    "                if key[-3:-1] == '_l']) + 1\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0\n",
    "try:\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "except:\n",
    "    PAD_IDX = 1\n",
    "\n",
    "# Instantiate\n",
    "model = RNN(RNN_TYPE, INPUT_DIM, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, \n",
    "            BIDIRECTIONAL, DROPOUT, PAD_IDX, TRAIN_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1597315290569,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "nsoE8DO6Vs2X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1597315290573,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "4XYr1tDIZn38"
   },
   "outputs": [],
   "source": [
    "\n",
    "if BINARY_LABELS:\n",
    "    # Binary cross-entropy loss with logits\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    # Binary accuracy\n",
    "    def accuracy(preds, y):\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "else:\n",
    "    # Cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # Categorical accuracy\n",
    "    def accuracy(preds, y):\n",
    "        \"\"\" Categorical accuracy for multiple classes.\"\"\"\n",
    "        max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(y)\n",
    "        return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
    "        \n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 344083,
     "status": "ok",
     "timestamp": 1597316421326,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "9AGbY9o-3lk1",
    "outputId": "95f34b26-7ad3-42b0-bfa0-fbd8ed1952f9"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Evaluate truncated networks\n",
    "max_rank = HIDDEN_DIM\n",
    "rank_step = HIDDEN_DIM // 64\n",
    "trunc_ranks = np.arange(0, max_rank, rank_step)\n",
    "n_rank = len(trunc_ranks)\n",
    "\n",
    "# Keys\n",
    "keys_rnn = model.keys_rnn\n",
    "n_states = len(keys_rnn)\n",
    "nhid = HIDDEN_DIM \n",
    "\n",
    "svd_blockwise = False\n",
    "\n",
    "############################################################################\n",
    "trunc_loss_acc = np.zeros((2, n_rank, 2))\n",
    "\n",
    "time0 = time.time()\n",
    "for j in range(2):\n",
    "    trunc_dw = j == 0\n",
    "    for i, rank in enumerate(trunc_ranks):\n",
    "        print(j, i, rank)\n",
    "\n",
    "        # Weights and biases for truncated model\n",
    "        state_dict_trunc = OrderedDict()\n",
    "        \n",
    "        # Weights\n",
    "        for key in state_dict_final.keys():\n",
    "            # Truncate only 'inner' weights with NxN\n",
    "            cond_trunc = (\n",
    "                ('rnn.weight' in key) \n",
    "                * (('hh' in key) or ('ih' in key))\n",
    "                * (not 'ih_l0' in key))\n",
    "            if cond_trunc:\n",
    "                # print(\"Truncate\", key)\n",
    "                if svd_blockwise:\n",
    "                    # LSTM and GRU cells have combined weights for the effect of the\n",
    "                    # last hidden state on the new state and the gates. \n",
    "                    w_trunc_rnn = np.zeros((n_states * nhid, nhid))\n",
    "                    for k, key_rnn in enumerate(keys_rnn):\n",
    "                        # Get weights\n",
    "                        w = state_dict_final[key][k*nhid : (k+1)*nhid]\n",
    "                        # Move to cpu\n",
    "                        w = w.cpu()\n",
    "                        if trunc_dw:\n",
    "                            # Obtain changes\n",
    "                            w0 = state_dict_init[key][k*nhid : (k+1)*nhid]\n",
    "                            w0 = w0.cpu()\n",
    "                            dw = w - w0\n",
    "                            # Truncate dw at given rank:\n",
    "                            u, s, vT = np.linalg.svd(dw, full_matrices=False)\n",
    "                            dw_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                            # Unite with initial connecitivity\n",
    "                            w_trunc_rnn[k*nhid : (k+1)*nhid] = w0 + dw_trunc\n",
    "                        else:\n",
    "                            # Truncate w at given rank:\n",
    "                            u, s, vT = np.linalg.svd(w, full_matrices=False)\n",
    "                            w_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                            w_trunc_rnn[k*nhid : (k+1)*nhid] = w_trunc\n",
    "                    # Add to state dict\n",
    "                    w_trunc_rnn = torch.from_numpy(w_trunc_rnn).cuda()\n",
    "                    state_dict_trunc[key] = w_trunc_rnn\n",
    "                else:\n",
    "                    # Get weights\n",
    "                    w = state_dict_final[key]\n",
    "                    # Move to cpu\n",
    "                    w = w.cpu()\n",
    "                    if trunc_dw:\n",
    "                        # Obtain changes\n",
    "                        w0 = state_dict_init[key]\n",
    "                        w0 = w0.cpu()\n",
    "                        dw = w - w0\n",
    "                        # Truncate dw at given rank:\n",
    "                        u, s, vT = np.linalg.svd(dw, full_matrices=False)\n",
    "                        dw_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                        # Unite with initial connecitivity\n",
    "                        w_trunc_rnn = w0.numpy() + dw_trunc\n",
    "                    else:\n",
    "                        # Truncate w at given rank:\n",
    "                        u, s, vT = np.linalg.svd(w, full_matrices=False)\n",
    "                        w_trunc = (u[:, :rank] * s[None, :rank]) @ vT[:rank]\n",
    "                        w_trunc_rnn = w_trunc\n",
    "                # Add to state dict\n",
    "                w_trunc_rnn = torch.from_numpy(w_trunc_rnn).cuda()\n",
    "                state_dict_trunc[key] = w_trunc_rnn\n",
    "            else:\n",
    "                # print(\"Leave\", key)\n",
    "                state_dict_trunc[key] = copy.deepcopy(state_dict_final[key])\n",
    "        \n",
    "        # Define truncated model\n",
    "        model_trunc = RNN(RNN_TYPE, INPUT_DIM, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                          N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, TRAIN_EMB)\n",
    "        model_trunc.load_state_dict(state_dict_trunc)\n",
    "        # Evaluate\n",
    "        model_trunc.to(device)\n",
    "        trunc_loss_acc[j, i] = evaluate(model_trunc, valid_iterator, criterion)\n",
    "        del model_trunc\n",
    "print('Computing truncation loss took %.1f sec.' % (time.time() - time0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1597315008133,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "dIjsTeMrZKM8",
    "outputId": "fc10af5a-aa35-4642-f7f0-0b5bab66d4f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1597316483472,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "H1zzbRpMxwU2",
    "outputId": "7902c5e8-3f7b-4c91-e33c-8803d16468be"
   },
   "outputs": [],
   "source": [
    "# Plot loss and accuracy over training epochs\n",
    "fig = plt.figure(figsize=(6, 6), facecolor='w')\n",
    "axes = fig.subplots(2, 1, sharex=True)\n",
    "axes = axes[:, None]\n",
    "epochs = np.arange(N_EPOCHS)\n",
    "train_loss, valid_loss, train_acc, valid_acc = loss_acc.T\n",
    "\n",
    "ax0, ax1 = axes[:, 0]\n",
    "\n",
    "for j in range(2):\n",
    "    # ls = ['-', '--'][j]\n",
    "    ls = '-'\n",
    "    lbl = ['Truncate dw', 'Truncate w'][j]\n",
    "    # c = colors[0]\n",
    "    c = ['0.3', '0.8'][j]\n",
    "\n",
    "    trunc_loss, trunc_acc = trunc_loss_acc[j].T\n",
    "    ax0.plot(trunc_ranks, trunc_loss, ls, c=c, label=lbl)\n",
    "    ax1.plot(trunc_ranks, trunc_acc, ls, c=c, label=lbl)\n",
    "\n",
    "# Initial and final loss and accuracy\n",
    "c = '0.5'\n",
    "ax0.axhline(valid_loss[0], ls='--', c=c, label='Initial')\n",
    "ax1.axhline(valid_acc[0], ls=':', c=c, label='Initial')\n",
    "ax0.axhline(valid_loss[-1], ls='--', c=c, label='Final')\n",
    "ax1.axhline(valid_acc[-1], ls=':', c=c, label='Final')\n",
    "\n",
    "ax0.legend()\n",
    "# ax0.axhline(0, c='0.5', zorder=-1)\n",
    "# ax1.axhline(0, c='0.5', zorder=-1)\n",
    "ax0.set_ylabel('Loss')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel(\"Truncation rank\")\n",
    "\n",
    "plt.show(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1597316497776,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "lXlfX_01LggQ",
    "outputId": "99c04279-9bdb-49d8-d92c-969532748c3c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save everything\n",
    "state_dict_final = model.state_dict()\n",
    "with open(SAVE, 'wb') as f:\n",
    "    torch.save({'state_dict_init': state_dict_init,\n",
    "                'state_dict_final': state_dict_final,\n",
    "                'loss_acc': loss_acc,\n",
    "                'trunc_loss_acc': trunc_loss_acc,\n",
    "                'trunc_ranks': trunc_ranks,\n",
    "                'svd_blockwise': svd_blockwise,\n",
    "                }, f)\n",
    "# print(\"Saved last model to '%s'\" % SAVE)\n",
    "print(\"Saved initial and final model to '%s'\" % SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 501275,
     "status": "ok",
     "timestamp": 1597259520916,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "r3SQVwED-uO0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97039,
     "status": "ok",
     "timestamp": 1597253655977,
     "user": {
      "displayName": "Fr Schu",
      "photoUrl": "",
      "userId": "09731992798374122010"
     },
     "user_tz": -180
    },
    "id": "55LexzG_-uO5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lnl1V2-D-uO-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "sentiment_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
