{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# RNN model and task\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "from model import RNN, train, run_net\n",
    "from task_generators import flipflop, mante, romo\n",
    "\n",
    "# Data directory for saving\n",
    "from data_dir import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shared parameters\n",
    "optimizer = 'adam'\n",
    "\n",
    "# Integration time step\n",
    "dt = 0.5\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "batch_size_test = 512\n",
    "# Neural noise\n",
    "noise_std = 0.\n",
    "# Whether IO vectors are orthogonalized\n",
    "orthogonalize_wio = False\n",
    "# Same random connectivity for each g?\n",
    "same_connectivity = False\n",
    "# Reconstruction loss\n",
    "n_ranks = 26\n",
    "\n",
    "# Network architecture\n",
    "ML_RNN = False\n",
    "nonlinearity = 'tanh'\n",
    "readout_nonlinearity = None\n",
    "\n",
    "# Task names\n",
    "tasks_file_name_prefix = [\"flipflop\", \"mante\", \"romo\"]\n",
    "# Number of epochs\n",
    "tasks_n_epochs = np.array([1000, 2000, 6000])\n",
    "# Number of kept weight matrices (we don't really need this for the analysis...)\n",
    "n_rec_epochs = 200\n",
    "\n",
    "# Learning rate\n",
    "lr0 = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = ['full', 'test'][1]\n",
    "\n",
    "if scenario == 'full':\n",
    "    # Network size\n",
    "    dim_rec = 256\n",
    "\n",
    "    # Simulate and analyze all three tasks for multiple samples. \n",
    "    n_samples = 5\n",
    "\n",
    "    # Values for g\n",
    "    dg = 0.3\n",
    "    tasks_g_max = [2.1, 2.7, 3.0]\n",
    "    tasks_gs = [np.arange(0, g_max+dg, dg) for g_max in tasks_g_max]\n",
    "\n",
    "    # What to train\n",
    "    train_wi=True\n",
    "    train_wrec=True\n",
    "    train_wo=True\n",
    "    train_brec=False\n",
    "\n",
    "elif scenario == 'test':\n",
    "    # For quick testing\n",
    "    dim_rec = 256\n",
    "    n_samples = 2\n",
    "    n_epochs = 20\n",
    "    tasks_n_epochs = np.array([n_epochs]*3)\n",
    "    n_rec_epochs = n_epochs\n",
    "    tasks_gs = [np.array([0., 1.8])]*3\n",
    "    tasks_file_name_prefix = [\"test_\" + fn for fn in tasks_file_name_prefix]\n",
    "    train_wi=False\n",
    "    train_wrec=True\n",
    "    train_wo=False\n",
    "    train_brec=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(task_specs):\n",
    "    (file_name_prefix, n_epochs, gs, dims, task_params, task_generator\n",
    "    ) = task_specs\n",
    "    # Task\n",
    "    task = task_generator(dims, dt, **task_params)\n",
    "    n_gs = len(gs)\n",
    "    dim_in, dim_rec, dim_out = dims\n",
    "    \n",
    "    # Epochs\n",
    "    rec_step = n_epochs // n_rec_epochs\n",
    "    epochs = np.arange(n_epochs)\n",
    "    rec_epochs = np.arange(0, n_epochs, rec_step)\n",
    "    \n",
    "    # Learning rate\n",
    "    if optimizer == 'sgd':\n",
    "        lr = lr0\n",
    "    elif optimizer == 'adam':\n",
    "        lr = lr0 / dim_rec\n",
    "    \n",
    "    # For rank truncation\n",
    "    ranks = np.arange(n_ranks) \n",
    "    rank_max = ranks[-1] + 1\n",
    "    \n",
    "    # Weights\n",
    "    wi_init_all = np.zeros((n_samples, n_gs, dim_in, dim_rec))\n",
    "    wrec_init_all = np.zeros((n_samples, n_gs, dim_rec, dim_rec))\n",
    "    wo_init_all = np.zeros((n_samples, n_gs, dim_rec, dim_out))\n",
    "    brec_init_all = np.zeros((n_samples, n_gs, dim_rec))\n",
    "    wi_last_all = np.zeros((n_samples, n_gs, dim_in, dim_rec))\n",
    "    wrec_last_all = np.zeros((n_samples, n_gs, dim_rec, dim_rec))\n",
    "    wo_last_all = np.zeros((n_samples, n_gs, dim_rec, dim_out))\n",
    "    brec_last_all = np.zeros((n_samples, n_gs, dim_rec))\n",
    "    if train_wi:\n",
    "        wis_all = np.zeros((n_samples, n_gs, n_rec_epochs, dim_in, dim_rec))\n",
    "    if train_wo:\n",
    "        wos_all = np.zeros((n_samples, n_gs, n_rec_epochs, dim_rec, dim_out))\n",
    "    if train_brec:\n",
    "        brecs_all = np.zeros((n_samples, n_gs, n_rec_epochs, dim_rec))\n",
    "    # Results\n",
    "    losses_all = np.zeros((n_samples, n_gs, n_epochs))\n",
    "    grad_norms_all = np.zeros((n_samples, n_gs, n_epochs))\n",
    "    sv_dw_all = np.zeros((n_samples, n_gs, n_rec_epochs, dim_rec))\n",
    "    loss_rr_all = np.zeros((n_samples, n_gs, n_ranks))\n",
    "    norm_diff_rr_all = np.zeros((n_samples, n_gs, n_ranks))\n",
    "    var_expl_all = np.zeros((n_samples, n_gs, n_ranks))\n",
    "    loss_shuff_all = np.zeros((n_samples, n_gs))\n",
    "    ev_all = np.zeros((3, n_samples, n_gs, dim_rec), dtype=complex)\n",
    "    sv_all = np.zeros((3, n_samples, n_gs, dim_rec))\n",
    "\n",
    "    # Figure and file name\n",
    "    file_name = file_name_prefix\n",
    "    if n_gs == 1:\n",
    "        g = gs[0]\n",
    "        file_name += \"_g_%.1f\" % g\n",
    "    else:\n",
    "        file_name += \"_gs\"\n",
    "    # RNN model\n",
    "    if ML_RNN:\n",
    "        file_name += \"_ML_RNN\"\n",
    "    if not (nonlinearity == 'tanh' or nonlinearity == None):\n",
    "        file_name += \"_%s\" % nonlinearity\n",
    "    if not (readout_nonlinearity == 'tanh' or readout_nonlinearity == None):\n",
    "        file_name += \"_%s\" % readout_nonlinearity\n",
    "    # Optimizer\n",
    "    if optimizer != 'sgd':\n",
    "        file_name += '_' + optimizer\n",
    "    # What is trained\n",
    "    file_name += \"_train\"\n",
    "    if train_wi:\n",
    "        file_name += \"_wi\"\n",
    "    if train_wrec:\n",
    "        file_name += \"_wrec\"\n",
    "    if train_wo:\n",
    "        file_name += \"_wo\"\n",
    "    if train_brec:\n",
    "        file_name += \"_brec\"\n",
    "    if same_connectivity:\n",
    "        file_name += \"_same_conn\"\n",
    "    if orthogonalize_wio:\n",
    "        file_name += \"_ortho_wio\"\n",
    "    if noise_std != 0:\n",
    "        file_name += \"_noise_%.1f\"%noise_std\n",
    "    # Network parameters\n",
    "    file_name += \"_N_%d\"%dim_rec\n",
    "    file_name += \"_lr0_%.2f\" % lr0\n",
    "    file_name = \"\".join(file_name.split('.'))\n",
    "    print(\"file_name:\\n\", file_name)\n",
    "    # Data file for saving\n",
    "    data_file_name = file_name + \".pkl\"\n",
    "    data_file = os.path.join(data_dir, data_file_name)\n",
    "    print(\"data_file:\\n\", data_file)\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        print(\"Sample\", k)\n",
    "        time_t = 0\n",
    "        time_sv = 0\n",
    "        time_r = 0\n",
    "        time_ls = 0\n",
    "        for i, g in enumerate(gs):\n",
    "            print(\"   \", i, g)\n",
    "            \n",
    "            if (not same_connectivity) or i == 0:\n",
    "                # Connectivity\n",
    "                # Initial internal connectivity\n",
    "                wrec_0 = np.random.normal(0, 1 / np.sqrt(dim_rec), (dim_rec, dim_rec))\n",
    "                # Input and output vectors\n",
    "                wio = np.random.normal(0, 1, (dim_rec, dim_in + dim_out))\n",
    "                if orthogonalize_wio:\n",
    "                    wio = np.linalg.qr(wio)[0]\n",
    "                else:\n",
    "                    wio /= np.linalg.norm(wio, axis=0)[None, :]\n",
    "                # Make sure that the vecotrs are still normalized\n",
    "                assert np.allclose(np.linalg.norm(wio, axis=0), 1), \"Normalization gone wrong!\"\n",
    "                # Change normalization to the proper one\n",
    "                wio *= np.sqrt(dim_rec)\n",
    "                wi_init = wio[:, :dim_in].T.copy()\n",
    "                wo_init = wio[:, dim_in:].copy() / dim_rec\n",
    "                del wio\n",
    "\n",
    "            wrec_init = g * wrec_0\n",
    "\n",
    "            # Network\n",
    "            net = RNN(dims, noise_std, dt, \n",
    "                      g=g, wi_init=wi_init, wo_init=wo_init, wrec_init=wrec_init,\n",
    "                      train_wi=train_wi, train_wrec=train_wrec, train_wo=train_wo, train_brec=train_brec,\n",
    "                      nonlinearity=nonlinearity, readout_nonlinearity=readout_nonlinearity, \n",
    "                      ML_RNN=ML_RNN,\n",
    "                     )\n",
    "\n",
    "            # Train\n",
    "            time0_t = time.time()\n",
    "            res = train(net, task, n_epochs, batch_size, lr, cuda=use_cuda, rec_step=rec_step, optimizer=optimizer, verbose=False)\n",
    "            losses, grad_norms, weights_init, weights_last, weights_train, _, _ = res\n",
    "            # Weights\n",
    "            wi_init, wrec_init, wo_init, brec_init = weights_init\n",
    "            wi_last, wrec_last, wo_last, brec_last = weights_last\n",
    "            dwrec_last = wrec_last - wrec_init\n",
    "            wrecs = weights_train[\"wrec\"]\n",
    "            time_t += time.time() - time0_t\n",
    "\n",
    "            # Compute SVs\n",
    "            time0_sv = time.time()\n",
    "            sv_dw = np.linalg.svd(wrecs - wrec_init, compute_uv=False)\n",
    "            del wrecs\n",
    "            time_sv += time.time() - time0_sv\n",
    "\n",
    "            # Reconstruct connectivty with only the largest rank\n",
    "            time0_r = time.time()\n",
    "            u_last, s_last, vT_last = np.linalg.svd(dwrec_last)\n",
    "            # Variance explained\n",
    "            cum_var_rr = np.r_[0, (s_last[:n_ranks - 1]**2).cumsum()]\n",
    "            var_expl = cum_var_rr / (s_last**2).sum()\n",
    "            # Simulate for truncated dwrec\n",
    "            loss_rr_i = np.zeros((n_ranks))\n",
    "            norm_diff_rr_i = np.zeros((n_ranks))\n",
    "            for j, rank in enumerate(ranks):\n",
    "                if rank == 0:\n",
    "                    dw_rr = 0\n",
    "                else:\n",
    "                    dw_rr = (u_last[:, :rank] * s_last[None, :rank]) @ vT_last[:rank]\n",
    "                w_rr = wrec_init + dw_rr\n",
    "\n",
    "                # Run network\n",
    "                net_test = RNN(dims, noise_std, dt, \n",
    "                               g=None, wi_init=wi_last, wo_init=wo_last, wrec_init=w_rr, brec_init=brec_last,\n",
    "                               nonlinearity=nonlinearity, readout_nonlinearity=readout_nonlinearity, \n",
    "                               ML_RNN=ML_RNN,\n",
    "                              )\n",
    "                res_test = run_net(net_test, task, batch_size=batch_size_test)\n",
    "                u, y, mask, z, loss = res_test\n",
    "\n",
    "                # Save\n",
    "                loss_rr_i[j] = loss\n",
    "                norm_diff_rr_i[j] = np.linalg.norm(dw_rr - dwrec_last)\n",
    "            time_r += time.time() - time0_r\n",
    "\n",
    "            ### Loss for shuffled network\n",
    "            time0_ls = time.time()\n",
    "            # Shuffle is in-place!  -> copy!\n",
    "            wrec_init_shuff = wrec_init.copy()\n",
    "            # Shuffle needs reshape\n",
    "            wrec_init_shuff = wrec_init_shuff.reshape((dim_rec**2,))\n",
    "            np.random.shuffle(wrec_init_shuff)\n",
    "            wrec_init_shuff = wrec_init_shuff.reshape((dim_rec, dim_rec))\n",
    "            wrec_shuff = wrec_init_shuff + dwrec_last\n",
    "            # Run network\n",
    "            net_shuff = RNN(dims, noise_std, dt, \n",
    "                            g=None, wi_init=wi_last, wo_init=wo_last, wrec_init=wrec_shuff, brec_init=brec_last,\n",
    "                            nonlinearity=nonlinearity, readout_nonlinearity=readout_nonlinearity, \n",
    "                            ML_RNN=ML_RNN,\n",
    "                           )\n",
    "            res_shuff = run_net(net_shuff, task, batch_size=batch_size_test)\n",
    "            _, _, _, _, loss_shuff = res_shuff\n",
    "            time_ls += time.time() - time0_ls\n",
    "\n",
    "            # Save\n",
    "            wi_init_all[k, i] = wi_init\n",
    "            wrec_init_all[k, i] = wrec_init\n",
    "            wo_init_all[k, i] = wo_init\n",
    "            brec_init_all[k, i] = brec_init\n",
    "            wi_last_all[k, i] = wi_last\n",
    "            wrec_last_all[k, i] = wrec_last\n",
    "            wo_last_all[k, i] = wo_last\n",
    "            brec_last_all[k, i] = brec_last\n",
    "            if train_wi:\n",
    "                wis_all[k, i] = weights_train[\"wi\"]\n",
    "            if train_wo:\n",
    "                wos_all[k, i] = weights_train[\"wo\"]\n",
    "            if train_brec:\n",
    "                brecs_all[k, i] = weights_train[\"brec\"]\n",
    "            losses_all[k, i] = losses\n",
    "            grad_norms_all[k, i] = grad_norms\n",
    "            sv_dw_all[k, i] = sv_dw\n",
    "            loss_rr_all[k, i] = loss_rr_i\n",
    "            norm_diff_rr_all[k, i] = norm_diff_rr_i\n",
    "            var_expl_all[k, i] = var_expl\n",
    "            loss_shuff_all[k, i] = loss_shuff\n",
    "            \n",
    "        print(\"Learning took %.1f sec.\" % (time_t))\n",
    "        print(\"SV evaluation took %.1f sec\" % (time_sv))\n",
    "        print(\"Computing reconstruction loss took %.1f sec\" % (time_r))\n",
    "        print(\"Computing loss after shuffle took %.1f sec\" % (time_ls))\n",
    "        \n",
    "    # Compute EVs and SVs at the end of training\n",
    "    time0_es = time.time()\n",
    "    ev_all[0] = np.linalg.eigvals(wrec_init_all)\n",
    "    ev_all[1] = np.linalg.eigvals(wrec_last_all)\n",
    "    ev_all[2] = np.linalg.eigvals(wrec_last_all - wrec_init_all)\n",
    "    sv_all[0] = np.linalg.svd(wrec_init_all, compute_uv=False)\n",
    "    sv_all[1] = np.linalg.svd(wrec_last_all, compute_uv=False)\n",
    "    sv_all[2] = np.linalg.svd(wrec_last_all - wrec_init_all, compute_uv=False)\n",
    "    time_es = time.time() - time0_es\n",
    "    print(\"Computing EVs&SVs for all samples took %.1f sec\" % (time_es))\n",
    "\n",
    "    ###############################################################################\n",
    "    # Save data\n",
    "    to_be_dumped = {\n",
    "        # Simulation parameters\n",
    "        \"dims\": dims, \n",
    "        \"dt\": dt, \n",
    "        \"gs\": gs, \n",
    "        \"lr\": lr,\n",
    "        \"noise_std\": noise_std,\n",
    "        \"ML_RNN\": ML_RNN,\n",
    "        \"nonlinearity\": nonlinearity,\n",
    "        \"readout_nonlinearity\": readout_nonlinearity,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"rec_step\": rec_step,\n",
    "        \"epochs\": epochs, \n",
    "        \"rec_epochs\": rec_epochs, \n",
    "        \"ranks\": ranks,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"batch_size_test\": batch_size_test,\n",
    "        \"train_wi\": train_wi,\n",
    "        \"train_wrec\": train_wrec,\n",
    "        \"train_wo\": train_wo,\n",
    "        \"train_brec\": train_brec,\n",
    "        # Task\n",
    "        \"task_params\": task_params,\n",
    "        # Weights\n",
    "        \"wi_init_all\": wi_init_all,\n",
    "        \"wrec_init_all\": wrec_init_all,\n",
    "        \"wo_init_all\": wo_init_all,\n",
    "        \"brec_init_all\": brec_init_all,\n",
    "        \"wi_last_all\": wi_last_all,\n",
    "        \"wrec_last_all\": wrec_last_all,\n",
    "        \"wo_last_all\": wo_last_all,\n",
    "        \"brec_last_all\": brec_last_all,\n",
    "        # Results\n",
    "        \"losses_all\": losses_all, \n",
    "        \"grad_norms_all\": grad_norms_all, \n",
    "        \"sv_dw_all\": sv_dw_all, \n",
    "        \"loss_rr_all\": loss_rr_all,\n",
    "        \"norm_diff_rr_all\": norm_diff_rr_all,\n",
    "        \"var_expl_all\": var_expl_all,\n",
    "        \"loss_shuff_all\": loss_shuff_all,\n",
    "        \"ev_all\": ev_all,\n",
    "        \"sv_all\": sv_all,\n",
    "        # Computation time\n",
    "        \"time_t\": time_t,\n",
    "        \"time_sv\": time_sv,\n",
    "        \"time_r\": time_r,\n",
    "        \"time_ls\": time_ls,\n",
    "        \"time_es\": time_es,\n",
    "    }\n",
    "    \n",
    "    if train_wi:\n",
    "        to_be_dumped[\"wis_all\"] =  wis_all\n",
    "    if train_wo:\n",
    "        to_be_dumped[\"wos_all\"] =  wos_all\n",
    "    if train_brec:\n",
    "        to_be_dumped[\"brecs_all\"] =  brecs_all\n",
    "        \n",
    "    with open(data_file, 'wb') as handle:\n",
    "        pickle.dump(to_be_dumped, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Saved data to \" + data_file + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipflop\n",
    "idx_task = 0\n",
    "file_name_prefix = tasks_file_name_prefix[idx_task]\n",
    "n_epochs = tasks_n_epochs[idx_task]\n",
    "gs = tasks_gs[idx_task]\n",
    "# Network parameters\n",
    "dim_in = 2\n",
    "dim_out = dim_in\n",
    "dims = [dim_in, dim_rec, dim_out]\n",
    "# Join\n",
    "flipflop_params = {\n",
    "    \"t_max\": 50,\n",
    "    \"fixation_duration\": 1,\n",
    "    \"stimulus_duration\": 1,\n",
    "    \"decision_delay_duration\": 5,\n",
    "    \"stim_delay_duration_min\": 5,\n",
    "    \"stim_delay_duration_max\": 25,\n",
    "    \"input_amp\": 1,\n",
    "    \"target_amp\": 0.5,\n",
    "    \"fixate\": False,}\n",
    "flipflop_specs = (file_name_prefix, n_epochs, gs, dims, flipflop_params, flipflop)\n",
    "\n",
    "# Mante\n",
    "idx_task = 1\n",
    "file_name_prefix = tasks_file_name_prefix[idx_task]\n",
    "n_epochs = tasks_n_epochs[idx_task]\n",
    "gs = tasks_gs[idx_task]\n",
    "# Network parameters\n",
    "dim_in = 2 * 2\n",
    "dim_out = 1\n",
    "dims = [dim_in, dim_rec, dim_out]\n",
    "# Join\n",
    "mante_params = {\n",
    "    \"choices\": np.arange(dim_in//2),\n",
    "    \"fixation_duration\":  3,\n",
    "    \"stimulus_duration\":  20,\n",
    "    \"delay_duration\":  5,\n",
    "    \"decision_duration\":  20,\n",
    "    \"input_amp\":  1.,\n",
    "    \"target_amp\":  0.5,\n",
    "    \"context_amp\":  1.,\n",
    "    \"rel_input_std\":  0.05,\n",
    "    \"coherences\":  np.array([-8, -4, -2, -1, 1, 2, 4, 8]) / 8.,\n",
    "    \"fixate\": True,}\n",
    "mante_specs = (file_name_prefix, n_epochs, gs, dims, mante_params, mante)\n",
    "\n",
    "# Romo\n",
    "idx_task = 2\n",
    "file_name_prefix = tasks_file_name_prefix[idx_task]\n",
    "n_epochs = tasks_n_epochs[idx_task]\n",
    "gs = tasks_gs[idx_task]\n",
    "# Network parameters\n",
    "dim_in = 1\n",
    "dim_out = 2\n",
    "dims = [dim_in, dim_rec, dim_out]\n",
    "# Join\n",
    "romo_params = {\n",
    "    \"fixation_duration\": 3,\n",
    "    \"stimulus_duration\": 1,\n",
    "    \"decision_delay_duration\": 5,\n",
    "    \"decision_duration\": 10,\n",
    "    \"stim_delay_duration_min\": 2,\n",
    "    \"stim_delay_duration_max\": 8,\n",
    "    \"input_amp_min\": 0.5,\n",
    "    \"input_amp_max\": 1.5,\n",
    "    \"min_input_diff\": 0.2,\n",
    "    \"target_amp\": 0.5,\n",
    "    \"fixate\": True}\n",
    "romo_specs = (file_name_prefix, n_epochs, gs, dims, romo_params, romo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name:\n",
      " test_flipflop_gs_adam_train_wrec_N_256_lr0_005\n",
      "data_file:\n",
      " ../data/test_flipflop_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "Sample 0\n",
      "    0 0.0\n",
      "    1 1.8\n",
      "Learning took 5.2 sec.\n",
      "SV evaluation took 0.3 sec\n",
      "Computing reconstruction loss took 13.6 sec\n",
      "Computing loss after shuffle took 0.5 sec\n",
      "Sample 1\n",
      "    0 0.0\n",
      "    1 1.8\n",
      "Learning took 2.2 sec.\n",
      "SV evaluation took 0.4 sec\n",
      "Computing reconstruction loss took 13.9 sec\n",
      "Computing loss after shuffle took 0.5 sec\n",
      "Computing EVs&SVs for all samples took 0.5 sec\n",
      "Saved data to ../data/test_flipflop_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "\n",
      "file_name:\n",
      " test_mante_gs_adam_train_wrec_N_256_lr0_005\n",
      "data_file:\n",
      " ../data/test_mante_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "Sample 0\n",
      "    0 0.0\n",
      "    1 1.8\n",
      "Learning took 2.5 sec.\n",
      "SV evaluation took 0.3 sec\n",
      "Computing reconstruction loss took 12.6 sec\n",
      "Computing loss after shuffle took 0.5 sec\n",
      "Sample 1\n",
      "    0 0.0\n",
      "    1 1.8\n",
      "Learning took 2.4 sec.\n",
      "SV evaluation took 0.3 sec\n",
      "Computing reconstruction loss took 11.9 sec\n",
      "Computing loss after shuffle took 0.5 sec\n",
      "Computing EVs&SVs for all samples took 0.4 sec\n",
      "Saved data to ../data/test_mante_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "\n",
      "file_name:\n",
      " test_romo_gs_adam_train_wrec_N_256_lr0_005\n",
      "data_file:\n",
      " ../data/test_romo_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "Sample 0\n",
      "    0 0.0\n",
      "    1 1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaniv/interplay_randomness_structure/notebooks/model.py:54: UserWarning: Nominal g and wrec_init disagree: g = 1.80, g_wrec = 1.81\n",
      "  warn(\"Nominal g and wrec_init disagree: g = %.2f, g_wrec = %.2f\" % (g, g_wrec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning took 1.3 sec.\n",
      "SV evaluation took 0.3 sec\n",
      "Computing reconstruction loss took 5.4 sec\n",
      "Computing loss after shuffle took 0.2 sec\n",
      "Sample 1\n",
      "    0 0.0\n",
      "    1 1.8\n",
      "Learning took 1.6 sec.\n",
      "SV evaluation took 0.3 sec\n",
      "Computing reconstruction loss took 5.8 sec\n",
      "Computing loss after shuffle took 0.2 sec\n",
      "Computing EVs&SVs for all samples took 0.4 sec\n",
      "Saved data to ../data/test_romo_gs_adam_train_wrec_N_256_lr0_005.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "tasks_specs = [flipflop_specs, mante_specs, romo_specs]\n",
    "for task_specs in tasks_specs:\n",
    "    run_training(task_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
